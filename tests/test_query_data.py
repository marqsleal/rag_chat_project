"""
Comprehensive unit tests for query_data.py module.
"""

from unittest.mock import Mock, patch

import pytest

from rag_project.query_data import (
    RAGQueryEngine,
    create_rag_engine,
    format_prompt,
    init_chroma,
    load_local_llama,
    validate_query_inputs,
)
from rag_project.rag_models import LLMConfig, RAGResponse


class TestLoadLocalLlama:
    """Test cases for load_local_llama function."""

    @patch('rag_project.query_data.Path')
    @patch('rag_project.query_data.CTransformers')
    @patch('rag_project.query_data.logger')
    def test_load_local_llama_success(self, mock_logger, mock_ctransformers, mock_path):
        """
        Testa o carregamento bem-sucedido de um modelo Llama local.
        
        Este teste verifica se a fun√ß√£o load_local_llama consegue carregar corretamente
        um modelo LLM local quando o arquivo existe e todas as configura√ß√µes s√£o v√°lidas.
        √â importante testar este cen√°rio pois √© o caso de uso principal da fun√ß√£o.
        
        Cen√°rio testado:
        - Arquivo do modelo existe no sistema
        - Configura√ß√£o v√°lida √© fornecida (temperatura e max_tokens)
        - CTransformers √© inicializado corretamente
        - Logs de informa√ß√£o s√£o gerados
        """
        mock_path_instance = Mock()
        mock_path_instance.exists.return_value = True
        mock_path.return_value = mock_path_instance

        mock_llm = Mock()
        mock_ctransformers.return_value = mock_llm

        config = LLMConfig(temperature=0.5, max_new_tokens=100)
        result = load_local_llama("test_model", "llama", config)

        assert result == mock_llm
        mock_ctransformers.assert_called_once_with(
            model="test_model",
            model_file=str(mock_path_instance),
            model_type="llama",
            config=config.model_dump(),
        )
        assert mock_logger.info.call_count >= 1

    @patch('rag_project.query_data.Path')
    @patch('rag_project.query_data.logger')
    def test_load_local_llama_file_not_found(self, mock_logger, mock_path):
        """
        Testa o comportamento quando o arquivo do modelo n√£o existe.
        
        Este teste √© crucial para verificar se a aplica√ß√£o trata adequadamente
        situa√ß√µes onde o usu√°rio especifica um caminho inv√°lido para o modelo.
        Garante que uma exce√ß√£o espec√≠fica (FileNotFoundError) seja lan√ßada
        com uma mensagem clara, evitando comportamentos inesperados.
        
        Cen√°rio testado:
        - Arquivo do modelo n√£o existe no sistema de arquivos
        - FileNotFoundError √© lan√ßada com mensagem apropriada
        - Log de erro √© gerado para debugging
        """
        mock_path_instance = Mock()
        mock_path_instance.exists.return_value = False
        mock_path.return_value = mock_path_instance
        
        with pytest.raises(FileNotFoundError) as exc_info:
            load_local_llama("nonexistent_model")
        
        assert "Model file not found" in str(exc_info.value)
        mock_logger.error.assert_called()

    @patch('rag_project.query_data.Path')
    @patch('rag_project.query_data.CTransformers')
    @patch('rag_project.query_data.logger')
    def test_load_local_llama_ctransformers_error(self, mock_logger, mock_ctransformers, mock_path):
        """
        Testa o tratamento de erros durante a inicializa√ß√£o do CTransformers.
        
        Este teste verifica se a aplica√ß√£o trata adequadamente falhas na
        inicializa√ß√£o da biblioteca CTransformers. Isso pode ocorrer por
        problemas de mem√≥ria, modelo corrompido, ou incompatibilidade de
        hardware. √â essencial para garantir que erros internos sejam
        propagados corretamente.
        
        Cen√°rio testado:
        - Arquivo do modelo existe mas CTransformers falha ao inicializar
        - RuntimeError √© propagada corretamente
        - Log de erro √© gerado para troubleshooting
        """
        mock_path_instance = Mock()
        mock_path_instance.exists.return_value = True
        mock_path.return_value = mock_path_instance
        
        mock_ctransformers.side_effect = RuntimeError("Initialization failed")
        
        with pytest.raises(RuntimeError):
            load_local_llama("test_model")
        
        mock_logger.error.assert_called()

    @patch('rag_project.query_data.Path')
    @patch('rag_project.query_data.CTransformers')
    @patch('rag_project.query_data.logger')
    def test_load_local_llama_with_default_config(self, mock_logger, mock_ctransformers, mock_path):
        """
        Testa o carregamento do modelo com configura√ß√µes padr√£o.
        
        Este teste verifica se a fun√ß√£o funciona corretamente quando
        nenhum par√¢metro √© fornecido, utilizando valores padr√£o.
        √â importante para garantir que a fun√ß√£o seja us√°vel sem
        configura√ß√£o manual detalhada.
        
        Cen√°rio testado:
        - Nenhum par√¢metro fornecido (usa defaults)
        - CTransformers √© chamado com configura√ß√£o padr√£o
        - Fun√ß√£o retorna objeto LLM v√°lido
        """
        mock_path_instance = Mock()
        mock_path_instance.exists.return_value = True
        mock_path.return_value = mock_path_instance
        
        mock_llm = Mock()
        mock_ctransformers.return_value = mock_llm
        
        result = load_local_llama()
        
        assert result == mock_llm
        mock_ctransformers.assert_called_once()

    @patch('rag_project.query_data.Path')
    @patch('rag_project.query_data.logger')
    def test_load_local_llama_permission_error(self, mock_logger, mock_path):
        """
        Testa o tratamento de erros de permiss√£o ao acessar o modelo.
        
        Este teste verifica o comportamento quando a aplica√ß√£o n√£o tem
        permiss√µes adequadas para acessar o arquivo do modelo. Isso pode
        ocorrer em sistemas com controle de acesso rigoroso ou quando
        o arquivo est√° sendo usado por outro processo.
        
        Cen√°rio testado:
        - Arquivo existe mas n√£o h√° permiss√£o de leitura
        - PermissionError √© lan√ßada corretamente
        - Sistema falha de forma controlada e previs√≠vel
        """
        mock_path_instance = Mock()
        mock_path_instance.exists.return_value = True
        mock_path.return_value = mock_path_instance
        
        with patch('rag_project.query_data.CTransformers') as mock_ctransformers:
            mock_ctransformers.side_effect = PermissionError("Permission denied")
            
            with pytest.raises(PermissionError):
                load_local_llama("test_model")


class TestInitChroma:
    """Test cases for init_chroma function."""

    @patch('rag_project.query_data.init_embeddings')
    @patch('rag_project.query_data.Chroma')
    @patch('rag_project.query_data.Path')
    @patch('rag_project.query_data.logger')
    def test_init_chroma_success_existing_dir(self, mock_logger, mock_path, mock_chroma, mock_init_embeddings):
        """
        Testa a inicializa√ß√£o bem-sucedida do Chroma com diret√≥rio existente.
        
        Este teste verifica se a fun√ß√£o init_chroma consegue inicializar
        corretamente o banco de dados vetorial Chroma quando o diret√≥rio
        de destino j√° existe. √â o cen√°rio mais comum em uso cont√≠nuo.
        
        Cen√°rio testado:
        - Diret√≥rio do banco de dados j√° existe
        - Embeddings s√£o inicializados corretamente
        - Chroma √© configurado com sucesso
        - N√£o tenta criar diret√≥rio existente
        """
        mock_embedding = Mock()
        mock_init_embeddings.return_value = mock_embedding

        mock_path_instance = Mock()
        mock_path_instance.exists.return_value = True
        mock_path.return_value = mock_path_instance

        mock_chroma_db = Mock()
        mock_chroma.return_value = mock_chroma_db

        result = init_chroma("/test/path", "test_collection")

        assert result == mock_chroma_db
        mock_chroma.assert_called_once()
        mock_path_instance.mkdir.assert_not_called()

    @patch('rag_project.query_data.init_embeddings')
    @patch('rag_project.query_data.Chroma')
    @patch('rag_project.query_data.Path')
    @patch('rag_project.query_data.logger')
    def test_init_chroma_success_create_dir(self, mock_logger, mock_path, mock_chroma, mock_init_embeddings):
        """
        Testa a inicializa√ß√£o do Chroma criando novo diret√≥rio.
        
        Este teste verifica se a fun√ß√£o consegue criar automaticamente
        o diret√≥rio de destino quando ele n√£o existe. √â importante para
        a primeira execu√ß√£o ou quando o usu√°rio especifica um novo local.
        
        Cen√°rio testado:
        - Diret√≥rio n√£o existe inicialmente
        - Diret√≥rio √© criado automaticamente (com parents=True)
        - Warning √© logado sobre cria√ß√£o do diret√≥rio
        - Chroma √© inicializado com sucesso ap√≥s cria√ß√£o
        """
        mock_embedding = Mock()
        mock_init_embeddings.return_value = mock_embedding
        
        mock_path_instance = Mock()
        mock_path_instance.exists.return_value = False
        mock_path.return_value = mock_path_instance
        
        mock_chroma_db = Mock()
        mock_chroma.return_value = mock_chroma_db
        
        result = init_chroma("/test/new_path", "test_collection")
        
        assert result == mock_chroma_db
        mock_path_instance.mkdir.assert_called_once_with(parents=True, exist_ok=True)
        mock_logger.warning.assert_called()

    @patch('rag_project.query_data.init_embeddings')
    @patch('rag_project.query_data.logger')
    def test_init_chroma_embeddings_error(self, mock_logger, mock_init_embeddings):
        """
        Testa o tratamento de falhas na inicializa√ß√£o dos embeddings.
        
        Este teste verifica se o sistema trata adequadamente erros durante
        a inicializa√ß√£o dos modelos de embedding, que s√£o essenciais para
        a busca sem√¢ntica. Falhas podem ocorrer por falta de mem√≥ria,
        modelos corrompidos ou problemas de conectividade.
        
        Cen√°rio testado:
        - Inicializa√ß√£o dos embeddings falha com RuntimeError
        - Erro √© propagado corretamente para o chamador
        - Log de erro √© gerado para debugging
        - Sistema falha de forma controlada
        """
        mock_init_embeddings.side_effect = RuntimeError("Embeddings failed")
        
        with pytest.raises(RuntimeError):
            init_chroma("/test/path", "test_collection")
        
        mock_logger.error.assert_called()

    @patch('rag_project.query_data.init_embeddings')
    @patch('rag_project.query_data.Chroma')
    @patch('rag_project.query_data.Path')
    @patch('rag_project.query_data.logger')
    def test_init_chroma_chroma_error(self, mock_logger, mock_path, mock_chroma, mock_init_embeddings):
        """
        Testa o tratamento de falhas na inicializa√ß√£o do banco Chroma.
        
        Este teste verifica se o sistema trata adequadamente erros durante
        a cria√ß√£o da inst√¢ncia do banco vetorial Chroma. Isso pode ocorrer
        por problemas de conectividade, configura√ß√£o incorreta, ou falhas
        no banco de dados subjacente.
        
        Cen√°rio testado:
        - Embeddings s√£o inicializados com sucesso
        - Diret√≥rio existe e est√° acess√≠vel
        - Chroma falha ao ser inicializado
        - RuntimeError √© propagada corretamente
        - Log de erro √© gerado para troubleshooting
        """
        mock_embedding = Mock()
        mock_init_embeddings.return_value = mock_embedding
        
        mock_path_instance = Mock()
        mock_path_instance.exists.return_value = True
        mock_path.return_value = mock_path_instance
        
        mock_chroma.side_effect = RuntimeError("Chroma failed")
        
        with pytest.raises(RuntimeError):
            init_chroma("/test/path", "test_collection")
        
        mock_logger.error.assert_called()

    @patch('rag_project.query_data.init_embeddings')
    @patch('rag_project.query_data.Path')
    @patch('rag_project.query_data.logger')
    def test_init_chroma_disk_full_error(self, mock_logger, mock_path, mock_init_embeddings):
        """
        Testa o tratamento de erros de espa√ßo em disco insuficiente.
        
        Este teste verifica se o sistema trata adequadamente situa√ß√µes
        onde n√£o h√° espa√ßo suficiente em disco para criar o diret√≥rio
        do banco vetorial. √â importante para ambientes com restri√ß√µes
        de armazenamento ou discos cheios.
        
        Cen√°rio testado:
        - Diret√≥rio n√£o existe e precisa ser criado
        - Sistema de arquivos est√° sem espa√ßo
        - OSError √© lan√ßada durante cria√ß√£o do diret√≥rio
        - Erro √© propagado sem tratamento especial
        """
        mock_path_instance = Mock()
        mock_path_instance.exists.return_value = False
        mock_path_instance.mkdir.side_effect = OSError("No space left on device")
        mock_path.return_value = mock_path_instance
        
        with pytest.raises(OSError):
            init_chroma("/test/path", "test_collection")


class TestFormatPrompt:
    """Test cases for format_prompt function."""

    @patch('rag_project.query_data.logger')
    def test_format_prompt_success(self, mock_logger):
        """
        Testa a formata√ß√£o bem-sucedida de prompts para o LLM.
        
        Este teste verifica se a fun√ß√£o format_prompt consegue combinar
        corretamente contexto, pergunta e template para criar um prompt
        estruturado. √â fundamental para garantir que o LLM receba
        informa√ß√µes no formato esperado.
        
        Cen√°rio testado:
        - Contexto e pergunta v√°lidos fornecidos
        - Template personalizado √© usado corretamente
        - String formatada retornada conforme esperado
        - Logs de debug/info s√£o gerados
        """
        context = "This is test context"
        question = "What is this about?"
        template = "Context: {context}\nQuestion: {question}"
        
        result = format_prompt(context, question, template)
        
        expected = "Context: This is test context\nQuestion: What is this about?"
        assert result == expected
        assert mock_logger.debug.called or mock_logger.info.called

    @patch('rag_project.query_data.logger')
    def test_format_prompt_empty_context_warning(self, mock_logger):
        """
        Testa o comportamento com contexto vazio gerando warnings.
        
        Este teste verifica se a fun√ß√£o detecta quando nenhum contexto
        relevante foi encontrado e gera alertas apropriados. √â importante
        porque contexto vazio pode indicar problemas na recupera√ß√£o de
        documentos ou configura√ß√£o inadequada do banco vetorial.
        
        Cen√°rio testado:
        - Contexto est√° vazio (string vazia)
        - Pergunta √© v√°lida
        - Warning √© logado sobre contexto vazio
        - Formata√ß√£o continua funcionando mesmo assim
        """
        context = ""
        question = "What is this about?"
        template = "Context: {context}\nQuestion: {question}"
        
        result = format_prompt(context, question, template)
        
        expected = "Context: \nQuestion: What is this about?"
        assert result == expected
        mock_logger.warning.assert_called()

    @patch('rag_project.query_data.logger')
    def test_format_prompt_whitespace_context_warning(self, mock_logger):
        """
        Testa a detec√ß√£o de contexto contendo apenas espa√ßos em branco.
        
        Este teste verifica se a fun√ß√£o detecta contextos que parecem
        n√£o-vazios mas cont√™m apenas caracteres de espa√ßamento (espa√ßos,
        tabs, quebras de linha). √â importante para identificar casos
        onde a recupera√ß√£o de documentos retorna conte√∫do sem valor real.
        
        Cen√°rio testado:
        - Contexto cont√©m apenas whitespace (espa√ßos, tabs, newlines)
        - Pergunta √© v√°lida
        - Warning √© logado sobre contexto efetivamente vazio
        - Sistema detecta "falso" conte√∫do
        """
        context = "   \n  \t  "
        question = "What is this about?"
        template = "Context: {context}\nQuestion: {question}"
        
        format_prompt(context, question, template)
        
        mock_logger.warning.assert_called()

    @patch('rag_project.query_data.logger')
    def test_format_prompt_empty_question_error(self, mock_logger):
        """
        Testa a valida√ß√£o rigorosa de perguntas vazias.
        
        Este teste verifica se a fun√ß√£o rejeita corretamente tentativas
        de formata√ß√£o com perguntas vazias. √â cr√≠tico porque uma pergunta
        vazia resultaria em consultas sem sentido ao LLM, desperdi√ßando
        recursos computacionais e fornecendo respostas in√∫teis.
        
        Cen√°rio testado:
        - Contexto v√°lido fornecido
        - Pergunta est√° vazia (string vazia)
        - ValueError √© lan√ßada com mensagem clara
        - Log de erro √© gerado para debugging
        """
        context = "Some context"
        question = ""
        template = "Context: {context}\nQuestion: {question}"
        
        with pytest.raises(ValueError) as exc_info:
            format_prompt(context, question, template)
        
        assert "Question cannot be empty" in str(exc_info.value)
        mock_logger.error.assert_called()

    @patch('rag_project.query_data.logger')
    def test_format_prompt_whitespace_question_error(self, mock_logger):
        """
        Testa a rejei√ß√£o de perguntas contendo apenas espa√ßos em branco.
        
        Este teste verifica se a fun√ß√£o detecta e rejeita perguntas que
        parecem n√£o-vazias mas cont√™m apenas caracteres de espa√ßamento.
        √â cr√≠tico para evitar processamento de "pseudo-perguntas" que
        n√£o t√™m conte√∫do real e desperdi√ßariam recursos.
        
        Cen√°rio testado:
        - Contexto v√°lido √© fornecido
        - Pergunta cont√©m apenas whitespace (espa√ßos, tabs, newlines)
        - ValueError √© lan√ßada tratando como pergunta vazia
        - Sistema n√£o aceita perguntas "falsamente" preenchidas
        """
        context = "Some context"
        question = "   \n  \t  "
        template = "Context: {context}\nQuestion: {question}"
        
        with pytest.raises(ValueError) as exc_info:
            format_prompt(context, question, template)
        
        assert "Question cannot be empty" in str(exc_info.value)

    @patch('rag_project.query_data.logger')
    def test_format_prompt_long_strings_truncated_in_logs(self, mock_logger):
        """
        Testa o manuseio de strings muito longas na formata√ß√£o.
        
        Este teste verifica se a fun√ß√£o consegue processar contextos
        e perguntas muito longos sem problemas de performance ou
        mem√≥ria. Tamb√©m testa se os logs s√£o adequadamente truncados
        para evitar polui√ß√£o do sistema de logging.
        
        Cen√°rio testado:
        - Contexto de 200 caracteres (muito longo)
        - Pergunta de 150 caracteres (muito longa)
        - Formata√ß√£o funciona corretamente
        - Resultado preserva todo o conte√∫do
        - Logs podem ser truncados para legibilidade
        """
        context = "A" * 200
        question = "B" * 150
        template = "Context: {context}\nQuestion: {question}"
        
        result = format_prompt(context, question, template)
        
        assert "A" * 200 in result
        assert "B" * 150 in result
        assert len(result) > 300

    def test_format_prompt_with_default_template(self):
        """
        Testa o uso do template padr√£o quando nenhum √© especificado.
        
        Este teste verifica se a fun√ß√£o utiliza corretamente um template
        padr√£o quando o usu√°rio n√£o fornece um personalizado. √â importante
        para garantir que o sistema seja us√°vel sem configura√ß√£o detalhada
        e que o template padr√£o seja adequado.
        
        Cen√°rio testado:
        - Contexto e pergunta v√°lidos fornecidos
        - Nenhum template personalizado especificado
        - Template padr√£o √© usado automaticamente
        - Resultado cont√©m contexto e pergunta formatados
        """
        context = "Test context"
        question = "Test question?"
        
        result = format_prompt(context, question)
        
        assert "Test context" in result
        assert "Test question?" in result

    def test_format_prompt_malformed_template(self):
        """
        Testa o tratamento de templates malformados com chaves inv√°lidas.
        
        Este teste verifica se a fun√ß√£o detecta e rejeita templates que
        cont√™m placeholders inv√°lidos ou inexistentes. √â importante para
        evitar falhas silenciosas quando usu√°rios fornecem templates
        personalizados incorretos.
        
        Cen√°rio testado:
        - Contexto e pergunta v√°lidos fornecidos
        - Template cont√©m placeholder inv√°lido ({invalid_key})
        - KeyError √© lan√ßada durante formata√ß√£o
        - Erro indica problema espec√≠fico no template
        """
        context = "Test context"
        question = "Test question"
        malformed_template = "Context: {context}\nQuestion: {invalid_key}"
        
        with pytest.raises(KeyError):
            format_prompt(context, question, malformed_template)

    def test_format_prompt_with_special_characters(self):
        """
        Testa o manuseio de caracteres especiais e unicode.
        
        Este teste verifica se a fun√ß√£o consegue processar corretamente
        texto contendo caracteres especiais, unicode, emojis e acentos.
        √â essencial para sistemas multil√≠ngues e conte√∫do internacional,
        garantindo que n√£o h√° problemas de encoding.
        
        Cen√°rio testado:
        - Contexto com caracteres gregos, chineses, √°rabes e emojis
        - Pergunta com acentos em portugu√™s e franc√™s
        - Formata√ß√£o preserva todos os caracteres especiais
        - Nenhum problema de encoding ou corrup√ß√£o de texto
        """
        context = "Special chars: Œ±Œ≤Œ≥ Œ¥ŒµŒ∂ Œ∑Œ∏Œπ ‰∏≠Êñá ÿßŸÑÿπÿ±ÿ®Ÿäÿ© üöÄü§ñ"
        question = "What about √©mojis and √±o√±√≥ characters?"
        template = "Context: {context}\nQuestion: {question}"
        
        with patch('rag_project.query_data.logger'):
            result = format_prompt(context, question, template)
        
        assert context in result
        assert question in result
        assert "üöÄü§ñ" in result


class TestValidateQueryInputs:
    """Test cases for validate_query_inputs function."""

    @patch('rag_project.query_data.logger')
    def test_validate_query_inputs_success(self, mock_logger):
        """
        Testa a valida√ß√£o bem-sucedida de entrada de consulta v√°lida.
        
        Este teste verifica se a fun√ß√£o validate_query_inputs aceita
        corretamente perguntas v√°lidas dentro dos limites estabelecidos.
        √â essencial para garantir que entradas leg√≠timas n√£o sejam
        rejeitadas incorretamente.
        
        Cen√°rio testado:
        - Pergunta v√°lida e n√£o vazia
        - Comprimento dentro do limite especificado
        - Nenhuma exce√ß√£o √© lan√ßada
        - Logs informativos podem ser gerados
        """
        question = "This is a valid question?"
        max_length = 100

        validate_query_inputs(question, max_length)

        assert mock_logger.info.call_count >= 0

    @patch('rag_project.query_data.logger')
    def test_validate_query_inputs_empty_question(self, mock_logger):
        """
        Testa a rejei√ß√£o de perguntas vazias durante valida√ß√£o.
        
        Este teste verifica se a fun√ß√£o detecta e rejeita perguntas vazias,
        que n√£o podem ser processadas de forma significativa pelo sistema RAG.
        √â fundamental para prevenir consultas inv√°lidas que desperdi√ßariam
        recursos computacionais.
        
        Cen√°rio testado:
        - String de pergunta completamente vazia
        - ValueError √© lan√ßada com mensagem espec√≠fica
        - Log de erro √© gerado para auditoria
        - Sistema falha de forma controlada
        """
        question = ""
        
        with pytest.raises(ValueError) as exc_info:
            validate_query_inputs(question)
        
        assert "Question cannot be empty" in str(exc_info.value)
        mock_logger.error.assert_called()

    @patch('rag_project.query_data.logger')
    def test_validate_query_inputs_none_question(self, mock_logger):
        """
        Testa a rejei√ß√£o de perguntas com valor None.
        
        Este teste verifica se a fun√ß√£o detecta e rejeita adequadamente
        valores None passados como pergunta. √â importante para prevenir
        erros de tipo e garantir que apenas strings v√°lidas sejam aceitas.
        
        Cen√°rio testado:
        - Pergunta √© None em vez de string
        - ValueError ou TypeError √© lan√ßada
        - Mensagem de erro indica problema de tipo ou valor vazio
        - Sistema n√£o aceita valores nulos
        """
        question = None
        
        with pytest.raises((ValueError, TypeError)) as exc_info:
            validate_query_inputs(question, 100)
        
        error_msg = str(exc_info.value)
        assert ("Question cannot be empty" in error_msg or 
                "expected str" in error_msg or 
                "NoneType" in error_msg)

    @patch('rag_project.query_data.logger')
    def test_validate_query_inputs_whitespace_question(self, mock_logger):
        """
        Testa a rejei√ß√£o de perguntas contendo apenas whitespace.
        
        Este teste verifica se a fun√ß√£o detecta perguntas que cont√™m
        apenas caracteres de espa√ßamento e as trata como vazias.
        √â importante para evitar processamento de perguntas sem
        conte√∫do real que desperdi√ßariam recursos.
        
        Cen√°rio testado:
        - Pergunta cont√©m apenas espa√ßos, tabs e quebras de linha
        - ValueError √© lan√ßada tratando como pergunta vazia
        - Sistema reconhece whitespace como conte√∫do inv√°lido
        - Valida√ß√£o √© rigorosa quanto ao conte√∫do real
        """
        question = "   \n  \t  "
        
        with pytest.raises(ValueError) as exc_info:
            validate_query_inputs(question, 100)
        
        assert "Question cannot be empty" in str(exc_info.value)

    @patch('rag_project.query_data.logger')
    def test_validate_query_inputs_too_long(self, mock_logger):
        """
        Testa a rejei√ß√£o de perguntas que excedem o limite de comprimento.
        
        Este teste verifica se a fun√ß√£o imp√µe corretamente limites no
        tamanho das perguntas para evitar problemas de performance,
        uso excessivo de mem√≥ria, ou limita√ß√µes do modelo LLM.
        
        Cen√°rio testado:
        - Pergunta com 1000 caracteres vs limite de 100
        - ValueError √© lan√ßada com mensagem espec√≠fica sobre limite
        - Log de erro √© gerado para auditoria
        - Sistema protege contra entrada excessivamente longa
        """
        question = "A" * 1000
        max_length = 100
        
        with pytest.raises(ValueError) as exc_info:
            validate_query_inputs(question, max_length)
        
        assert f"Maximum {max_length} characters allowed" in str(exc_info.value)
        mock_logger.error.assert_called()

    def test_validate_query_inputs_exact_max_length(self):
        """
        Testa a aceita√ß√£o de perguntas no limite exato de comprimento.
        
        Este teste verifica se a fun√ß√£o aceita corretamente perguntas
        que t√™m exatamente o comprimento m√°ximo permitido. √â importante
        para garantir que a valida√ß√£o de limite seja inclusiva e n√£o
        rejeite entradas leg√≠timas no limite.
        
        Cen√°rio testado:
        - Pergunta com exatamente 50 caracteres (limite = 50)
        - Nenhuma exce√ß√£o √© lan√ßada
        - Valida√ß√£o aceita comprimento no limite
        - Boundary testing para evitar off-by-one errors
        """
        max_length = 50
        question = "A" * max_length
        
        validate_query_inputs(question, max_length)

    def test_validate_query_inputs_with_default_max_length(self):
        """
        Testa o uso do comprimento m√°ximo padr√£o quando n√£o especificado.
        
        Este teste verifica se a fun√ß√£o utiliza corretamente um limite
        padr√£o de comprimento quando o usu√°rio n√£o especifica um valor.
        √â importante para garantir usabilidade sem configura√ß√£o detalhada.
        
        Cen√°rio testado:
        - Pergunta v√°lida de tamanho moderado
        - Nenhum max_length explicitamente fornecido
        - Fun√ß√£o usa limite padr√£o interno
        - Valida√ß√£o funciona sem par√¢metros opcionais
        """
        question = "Valid question"
        
        validate_query_inputs(question)

    def test_validate_query_inputs_exact_boundary(self):
        """
        Testa o comportamento exato nos limites de comprimento.
        
        Este teste verifica comportamentos boundary (lim√≠trofes) para
        garantir que a valida√ß√£o funciona corretamente tanto no limite
        quanto um caractere al√©m do limite. √â cr√≠tico para evitar
        off-by-one errors na valida√ß√£o.
        
        Cen√°rio testado:
        - Pergunta com exatamente 100 caracteres (aceita)
        - Pergunta com 101 caracteres (rejeitada)
        - Diferen√ßa de um caractere tem comportamento correto
        - Boundary testing abrangente
        """
        max_length = 100
        
        question_at_limit = "A" * max_length
        validate_query_inputs(question_at_limit, max_length)
        
        question_over_limit = "A" * (max_length + 1)
        with pytest.raises(ValueError):
            validate_query_inputs(question_over_limit, max_length)

    def test_validate_query_inputs_with_unicode(self):
        """
        Testa a valida√ß√£o de perguntas com caracteres unicode.
        
        Este teste verifica se a fun√ß√£o aceita corretamente perguntas
        contendo caracteres especiais, unicode e emojis. √â essencial
        para suporte internacional e multil√≠ngue do sistema RAG.
        
        Cen√°rio testado:
        - Pergunta com caracteres espanh√≥is, chineses, √°rabes e emoji
        - Nenhuma exce√ß√£o √© lan√ßada
        - Caracteres unicode s√£o tratados corretamente
        - Sistema suporta entrada multil√≠ngue
        """
        unicode_question = "Qu√© es esto? ËøôÊòØ‰ªÄ‰πàÔºüŸÖÿß Ÿáÿ∞ÿßÿü ü§î"
        
        with patch('rag_project.query_data.logger'):
            validate_query_inputs(unicode_question)


class TestRAGQueryEngine:
    """Test cases for RAGQueryEngine class."""

    def setup_method(self):
        self.mock_llm = Mock()
        self.mock_chroma = Mock()
        self.engine_config = {
            "model": "test_model",
            "model_type": "llama",
            "chroma_path": "/test/path",
            "collection_name": "test_collection",
            "llm_config": {"temperature": 0.5},
        }

    @patch('rag_project.query_data.logger')
    def test_rag_query_engine_init_success(self, mock_logger):
        """
        Testa a inicializa√ß√£o bem-sucedida do RAGQueryEngine.
        
        Este teste verifica se o engine principal do sistema RAG √©
        inicializado corretamente com todos os componentes necess√°rios
        (LLM e banco vetorial). √â fundamental para garantir que o
        sistema esteja pronto para processar consultas.
        
        Cen√°rio testado:
        - Componentes LLM e Chroma s√£o atribu√≠dos corretamente
        - Configura√ß√µes s√£o armazenadas adequadamente
        - Metadados do modelo s√£o preservados
        - Log de inicializa√ß√£o √© gerado
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        assert engine.llm == self.mock_llm
        assert engine.chroma == self.mock_chroma
        assert engine.model == "test_model"
        assert engine.model_type == "llama"
        assert engine.chroma_path == "/test/path"
        assert engine.collection_name == "test_collection"
        assert engine.llm_config == {"temperature": 0.5}
        
        mock_logger.info.assert_called()

    @patch('rag_project.query_data.logger')
    def test_retrieve_documents_success(self, mock_logger):
        """
        Testa a recupera√ß√£o bem-sucedida de documentos relevantes.
        
        Este teste verifica se o engine consegue buscar documentos
        similares √† consulta usando busca sem√¢ntica. √â o n√∫cleo do
        sistema RAG, respons√°vel por encontrar contexto relevante
        para fundamentar as respostas do LLM.
        
        Cen√°rio testado:
        - Busca sem√¢ntica retorna documentos com scores
        - Documentos acima do threshold s√£o inclu√≠dos
        - N√∫mero correto de documentos √© retornado
        - Chroma √© chamado com par√¢metros corretos
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        mock_doc1 = Mock()
        mock_doc2 = Mock()
        mock_results = [(mock_doc1, 0.8), (mock_doc2, 0.6)]
        
        self.mock_chroma.similarity_search_with_relevance_scores.return_value = mock_results
        
        result = engine.retrieve_documents("test question", documents_retrieve=2, min_similarity_score=0.5)
        
        assert len(result) == 2
        assert mock_doc1 in result
        assert mock_doc2 in result
        
        self.mock_chroma.similarity_search_with_relevance_scores.assert_called_once_with(
            "test question", k=2
        )

    @patch('rag_project.query_data.logger')
    def test_retrieve_documents_filtered_by_score(self, mock_logger):
        """
        Testa a filtragem de documentos por score de similaridade.
        
        Este teste verifica se o sistema consegue filtrar documentos
        baseado no threshold de similaridade, mantendo apenas os mais
        relevantes. √â crucial para evitar que informa√ß√µes pouco
        relacionadas contaminem o contexto fornecido ao LLM.
        
        Cen√°rio testado:
        - M√∫ltiplos documentos com scores diferentes
        - Apenas documentos acima do threshold s√£o retornados
        - Filtragem funciona corretamente (0.9 e 0.7 inclu√≠dos, 0.4 exclu√≠do)
        - Qualidade do contexto √© preservada
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        mock_doc1 = Mock()
        mock_doc2 = Mock()
        mock_doc3 = Mock()
        mock_results = [(mock_doc1, 0.9), (mock_doc2, 0.4), (mock_doc3, 0.7)]
        
        self.mock_chroma.similarity_search_with_relevance_scores.return_value = mock_results
        
        result = engine.retrieve_documents("test question", min_similarity_score=0.5)
        
        assert len(result) == 2
        assert mock_doc1 in result
        assert mock_doc3 in result
        assert mock_doc2 not in result

    @patch('rag_project.query_data.logger')
    def test_retrieve_documents_invalid_count(self, mock_logger):
        """
        Testa a valida√ß√£o de quantidade de documentos inv√°lida.
        
        Este teste verifica se o engine rejeita corretamente tentativas
        de recuperar zero ou n√∫meros negativos de documentos. √â importante
        para prevenir comportamentos indefinidos e garantir que apenas
        quantidades v√°lidas sejam processadas.
        
        Cen√°rio testado:
        - Solicita√ß√£o de 0 documentos (inv√°lido)
        - ValueError √© lan√ßada com mensagem espec√≠fica
        - Sistema valida par√¢metros antes da busca
        - Preven√ß√£o de consultas sem sentido
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        with pytest.raises(ValueError) as exc_info:
            engine.retrieve_documents("test question", documents_retrieve=0)
        
        assert "documents_retrieve must be positive" in str(exc_info.value)

    @patch('rag_project.query_data.logger')
    def test_retrieve_documents_chroma_error(self, mock_logger):
        """
        Testa o tratamento de erros durante busca no Chroma.
        
        Este teste verifica se o engine trata adequadamente falhas
        que podem ocorrer durante a busca sem√¢ntica no banco vetorial.
        Isso pode incluir problemas de conectividade, corrup√ß√£o de
        dados, ou falhas no √≠ndice vetorial.
        
        Cen√°rio testado:
        - Chroma falha durante similarity_search_with_relevance_scores
        - RuntimeError √© propagada corretamente
        - Log de erro √© gerado para debugging
        - Sistema falha de forma controlada
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        self.mock_chroma.similarity_search_with_relevance_scores.side_effect = RuntimeError("Search failed")
        
        with pytest.raises(RuntimeError):
            engine.retrieve_documents("test question")
        
        mock_logger.error.assert_called()

    @patch('rag_project.query_data.logger')
    def test_retrieve_documents_with_scores_success(self, mock_logger):
        """
        Testa a recupera√ß√£o de documentos junto com seus scores de similaridade.
        
        Este teste verifica se o engine consegue retornar tanto os documentos
        quanto seus scores de relev√¢ncia separadamente. √â √∫til para an√°lise
        de qualidade dos resultados e debugging do sistema de busca.
        
        Cen√°rio testado:
        - Busca retorna documentos com scores espec√≠ficos
        - M√©todo retorna tupla (documentos, scores)
        - Listas t√™m o mesmo comprimento
        - Scores correspondem aos documentos corretos
        - Filtragem por threshold funciona adequadamente
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        mock_doc1 = Mock()
        mock_doc2 = Mock()
        mock_results = [(mock_doc1, 0.8), (mock_doc2, 0.6)]
        
        self.mock_chroma.similarity_search_with_relevance_scores.return_value = mock_results
        
        docs, scores = engine.retrieve_documents_with_scores("test question", min_similarity_score=0.5)
        
        assert len(docs) == 2
        assert len(scores) == 2
        assert docs == [mock_doc1, mock_doc2]
        assert scores == [0.8, 0.6]

    @patch('rag_project.query_data.logger')
    def test_retrieve_documents_zero_similarity_threshold(self, mock_logger):
        """
        Testa o comportamento com threshold de similaridade zero.
        
        Este teste verifica se o engine aceita corretamente todos os
        documentos quando o threshold √© definido como 0.0, efetivamente
        desabilitando a filtragem por similaridade. √â √∫til para casos
        onde se deseja todos os resultados independente da relev√¢ncia.
        
        Cen√°rio testado:
        - Documentos com scores muito baixos (0.01, 0.001)
        - Threshold definido como 0.0 (aceita tudo)
        - Todos os documentos s√£o inclu√≠dos no resultado
        - Filtragem √© efetivamente desabilitada
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        mock_doc1 = Mock()
        mock_doc2 = Mock()
        mock_results = [(mock_doc1, 0.01), (mock_doc2, 0.001)]
        
        self.mock_chroma.similarity_search_with_relevance_scores.return_value = mock_results
        
        result = engine.retrieve_documents("test question", min_similarity_score=0.0)
        
        assert len(result) == 2

    @patch('rag_project.query_data.logger')
    def test_retrieve_documents_perfect_similarity_threshold(self, mock_logger):
        """
        Testa o comportamento com threshold de similaridade perfeita.
        
        Este teste verifica se o engine rejeita corretamente todos os
        documentos quando o threshold √© definido como 1.0 (similaridade
        perfeita). √â √∫til para casos onde apenas matches exatos s√£o
        desejados, embora seja raro na pr√°tica.
        
        Cen√°rio testado:
        - Documentos com scores altos mas n√£o perfeitos (0.99, 0.98)
        - Threshold definido como 1.0 (apenas matches perfeitos)
        - Todos os documentos s√£o rejeitados
        - Filtragem √© extremamente restritiva
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        mock_doc1 = Mock()
        mock_doc2 = Mock()
        mock_results = [(mock_doc1, 0.99), (mock_doc2, 0.98)]
        
        self.mock_chroma.similarity_search_with_relevance_scores.return_value = mock_results
        
        result = engine.retrieve_documents("test question", min_similarity_score=1.0)
        
        assert len(result) == 0

    @patch('rag_project.query_data.logger')
    def test_generate_answer_success(self, mock_logger):
        """
        Testa a gera√ß√£o bem-sucedida de resposta pelo LLM.
        
        Este teste verifica se o engine consegue solicitar corretamente
        ao LLM que gere uma resposta baseada no prompt formatado.
        √â a etapa final do pipeline RAG onde o conhecimento recuperado
        √© sintetizado em uma resposta coerente.
        
        Cen√°rio testado:
        - LLM recebe prompt formatado corretamente
        - Resposta √© gerada com sucesso
        - Resultado √© retornado sem modifica√ß√µes
        - Integra√ß√£o com o modelo funciona adequadamente
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        self.mock_llm.invoke.return_value = "Generated answer"
        
        result = engine.generate_answer("test prompt")
        
        assert result == "Generated answer"
        self.mock_llm.invoke.assert_called_once_with("test prompt")

    @patch('rag_project.query_data.logger')
    def test_generate_answer_error(self, mock_logger):
        """
        Testa o tratamento de erros durante gera√ß√£o de resposta pelo LLM.
        
        Este teste verifica se o engine trata adequadamente falhas que
        podem ocorrer durante a invoca√ß√£o do modelo LLM. Isso pode incluir
        problemas de mem√≥ria, modelo corrompido, timeout, ou falhas de
        conectividade (para modelos remotos).
        
        Cen√°rio testado:
        - LLM falha durante invoke() com RuntimeError
        - Erro √© propagado corretamente para o chamador
        - Log de erro √© gerado para debugging
        - Sistema falha de forma controlada sem corrup√ß√£o
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        self.mock_llm.invoke.side_effect = RuntimeError("Generation failed")
        
        with pytest.raises(RuntimeError):
            engine.generate_answer("test prompt")
        
        mock_logger.error.assert_called()

    @patch('rag_project.query_data.validate_query_inputs')
    @patch('rag_project.query_data.format_prompt')
    @patch('rag_project.query_data.logger')
    def test_query_success(self, mock_logger, mock_format_prompt, mock_validate):
        """
        Testa o fluxo completo de consulta RAG bem-sucedida.
        
        Este teste verifica todo o pipeline RAG integrado: valida√ß√£o
        da entrada, recupera√ß√£o de documentos, formata√ß√£o do prompt
        e gera√ß√£o da resposta. √â o teste mais importante pois simula
        o uso real do sistema.
        
        Cen√°rio testado:
        - Pergunta √© validada corretamente
        - Documentos relevantes s√£o recuperados
        - Prompt √© formatado com contexto e pergunta
        - LLM gera resposta baseada no contexto
        - Resposta √© limpa (whitespace removido)
        - Todas as etapas s√£o executadas na ordem correta
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        mock_doc = Mock()
        mock_doc.page_content = "Document content"
        mock_results = [(mock_doc, 0.8)]
        
        self.mock_chroma.similarity_search_with_relevance_scores.return_value = mock_results
        mock_format_prompt.return_value = "formatted prompt"
        self.mock_llm.invoke.return_value = "  Generated answer  "
        
        result = engine.query("test question")
        
        assert result == "Generated answer"
        mock_validate.assert_called_once()
        mock_format_prompt.assert_called_once()
        self.mock_llm.invoke.assert_called_once_with("formatted prompt")

    @patch('rag_project.query_data.validate_query_inputs')
    @patch('rag_project.query_data.logger')
    def test_query_no_documents_found(self, mock_logger, mock_validate):
        """
        Testa o comportamento quando nenhum documento relevante √© encontrado.
        
        Este teste verifica se o sistema trata adequadamente situa√ß√µes
        onde a busca sem√¢ntica n√£o retorna documentos suficientemente
        similares. √â importante para evitar alucina√ß√µes do LLM quando
        n√£o h√° contexto adequado dispon√≠vel.
        
        Cen√°rio testado:
        - Busca no banco vetorial n√£o retorna resultados
        - Sistema reconhece aus√™ncia de contexto relevante
        - Resposta padr√£o informativa √© retornada
        - Warning √© logado para indicar problema potencial
        - LLM n√£o √© chamado desnecessariamente
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        self.mock_chroma.similarity_search_with_relevance_scores.return_value = []
        
        result = engine.query("test question")
        
        expected = "I couldn't find any relevant information to answer your question."
        assert result == expected
        mock_logger.warning.assert_called()

    @patch('rag_project.query_data.validate_query_inputs')
    @patch('rag_project.query_data.format_prompt')
    @patch('rag_project.query_data.logger')
    def test_query_with_metadata_success(self, mock_logger, mock_format_prompt, mock_validate):
        """
        Testa a consulta com retorno de metadados detalhados.
        
        Este teste verifica se o engine consegue executar uma consulta
        completa e retornar n√£o apenas a resposta, mas tamb√©m metadados
        ricos incluindo fontes, scores de similaridade e estat√≠sticas.
        √â essencial para an√°lise de qualidade e rastreabilidade.
        
        Cen√°rio testado:
        - Consulta bem-sucedida com documento relevante
        - RAGResponse √© retornada com todos os campos preenchidos
        - Metadados incluem fonte, scores e estat√≠sticas
        - Pergunta original √© preservada para auditoria
        - Sistema fornece transpar√™ncia completa do processo
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        mock_doc = Mock()
        mock_doc.page_content = "Document content"
        mock_doc.metadata = {"source": "test_source.txt"}
        mock_results = [(mock_doc, 0.8)]
        
        self.mock_chroma.similarity_search_with_relevance_scores.return_value = mock_results
        mock_format_prompt.return_value = "formatted prompt"
        self.mock_llm.invoke.return_value = "Generated answer"
        
        result = engine.query_with_metadata("test question")
        
        assert isinstance(result, RAGResponse)
        assert result.answer == "Generated answer"
        assert result.sources == ["test_source.txt"]
        assert result.retrieved_docs == 1
        assert result.similarity_scores == [0.8]
        assert result.query == "test question"

    @patch('rag_project.query_data.validate_query_inputs')
    @patch('rag_project.query_data.logger')
    def test_query_with_metadata_no_documents(self, mock_logger, mock_validate):
        """
        Testa consulta com metadados quando nenhum documento √© encontrado.
        
        Este teste verifica se o engine retorna adequadamente metadados
        vazios quando nenhum documento relevante √© encontrado, mantendo
        a estrutura de resposta consistente mesmo em casos de falha
        na recupera√ß√£o de contexto.
        
        Cen√°rio testado:
        - Busca n√£o retorna documentos relevantes
        - RAGResponse √© retornada com campos vazios apropriados
        - Mensagem padr√£o √© fornecida como resposta
        - Metadados indicam claramente aus√™ncia de contexto
        - Estrutura de resposta permanece consistente
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        self.mock_chroma.similarity_search_with_relevance_scores.return_value = []
        
        result = engine.query_with_metadata("test question")
        
        assert isinstance(result, RAGResponse)
        assert "couldn't find any relevant information" in result.answer
        assert result.sources == []
        assert result.retrieved_docs == 0
        assert result.similarity_scores == []

    @patch('rag_project.query_data.logger')
    def test_get_config_info(self, mock_logger):
        """
        Testa a recupera√ß√£o de informa√ß√µes de configura√ß√£o do engine.
        
        Este teste verifica se o engine consegue retornar corretamente
        suas configura√ß√µes internas para debugging, monitoramento e
        auditoria. √â √∫til para verificar configura√ß√µes em tempo de
        execu√ß√£o e troubleshooting.
        
        Cen√°rio testado:
        - Engine retorna dicion√°rio com configura√ß√µes completas
        - Todas as configura√ß√µes principais est√£o presentes
        - Valores correspondem aos configurados na inicializa√ß√£o
        - Informa√ß√µes s√£o √∫teis para debugging e auditoria
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        result = engine.get_config_info()
        
        expected = {
            "model": "test_model",
            "model_type": "llama",
            "chroma_path": "/test/path",
            "collection_name": "test_collection",
            "llm_config": {"temperature": 0.5},
        }
        
        assert result == expected

    def test_repr(self):
        """
        Testa a representa√ß√£o string do engine para debugging.
        
        Este teste verifica se o engine fornece uma representa√ß√£o
        textual √∫til quando usado em debugging, logging ou inspe√ß√£o
        interativa. √â importante para facilitar o desenvolvimento
        e troubleshooting.
        
        Cen√°rio testado:
        - __repr__ retorna string informativa
        - String inclui identificadores chave (modelo e cole√ß√£o)
        - Formato √© consistente e leg√≠vel
        - √ötil para debugging e logging
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        result = repr(engine)
        
        expected = "RAGQueryEngine(model='test_model', collection='test_collection')"
        assert result == expected

    def test_rag_query_engine_corrupted_config(self):
        """
        Testa o comportamento com configura√ß√£o incompleta ou corrompida.
        
        Este teste verifica se o engine consegue lidar adequadamente
        com configura√ß√µes incompletas, definindo valores padr√£o ou None
        para campos ausentes. √â importante para robustez quando
        configura√ß√µes s√£o parcialmente corrompidas ou incompletas.
        
        Cen√°rio testado:
        - Configura√ß√£o cont√©m apenas o campo "model"
        - Campos ausentes s√£o definidos como None
        - Engine n√£o falha durante inicializa√ß√£o
        - Sistema √© tolerante a configura√ß√µes incompletas
        """
        mock_llm = Mock()
        mock_chroma = Mock()
        
        incomplete_config = {"model": "test_model"}
        
        with patch('rag_project.query_data.logger'):
            engine = RAGQueryEngine(mock_llm, mock_chroma, incomplete_config)
            
            assert engine.model == "test_model"
            assert engine.model_type is None
            assert engine.chroma_path is None

    def test_multiple_engine_instances(self):
        """
        Testa a cria√ß√£o de m√∫ltiplas inst√¢ncias independentes do engine.
        
        Este teste verifica se √© poss√≠vel criar v√°rias inst√¢ncias do
        RAGQueryEngine simultaneamente com configura√ß√µes diferentes,
        garantindo isolamento adequado entre elas. √â importante para
        aplica√ß√µes que precisam de m√∫ltiplos engines especializados.
        
        Cen√°rio testado:
        - Duas inst√¢ncias com configura√ß√µes completamente diferentes
        - Cada inst√¢ncia mant√©m sua pr√≥pria configura√ß√£o independente
        - N√£o h√° interfer√™ncia entre inst√¢ncias
        - LLMs e bancos vetorias s√£o isolados corretamente
        """
        mock_llm1 = Mock()
        mock_chroma1 = Mock()
        mock_llm2 = Mock()
        mock_chroma2 = Mock()
        
        config1 = {"model": "model1", "model_type": "type1", "chroma_path": "path1",
                  "collection_name": "collection1", "llm_config": {}}
        config2 = {"model": "model2", "model_type": "type2", "chroma_path": "path2",
                  "collection_name": "collection2", "llm_config": {}}
        
        with patch('rag_project.query_data.logger'):
            engine1 = RAGQueryEngine(mock_llm1, mock_chroma1, config1)
            engine2 = RAGQueryEngine(mock_llm2, mock_chroma2, config2)
        
        assert engine1.model != engine2.model
        assert engine1.chroma_path != engine2.chroma_path
        assert engine1.llm != engine2.llm

    def test_engine_large_context_memory_usage(self):
        """
        Testa o manuseio de contexto muito grande e uso de mem√≥ria.
        
        Este teste verifica se o engine consegue processar documentos
        com conte√∫do extremamente grande (100.000 caracteres) sem
        problemas de performance ou mem√≥ria. √â cr√≠tico para garantir
        escalabilidade com documentos grandes na vida real.
        
        Cen√°rio testado:
        - Documento com 100.000 caracteres √© processado
        - Sistema n√£o falha ou corrompe com contexto massivo
        - Mem√≥ria √© gerenciada adequadamente
        - Resposta √© gerada normalmente
        - Performance se mant√©m aceit√°vel
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        large_content = "A" * 100000
        mock_doc = Mock()
        mock_doc.page_content = large_content
        mock_doc.metadata = {"source": "large_doc.txt"}
        
        self.mock_chroma.similarity_search_with_relevance_scores.return_value = [(mock_doc, 0.8)]
        self.mock_llm.invoke.return_value = "Response to large context"
        
        with patch('rag_project.query_data.logger'), \
             patch('rag_project.query_data.format_prompt') as mock_format, \
             patch('rag_project.query_data.validate_query_inputs'):
            
            mock_format.return_value = "formatted_prompt"
            
            result = engine.query("test question")
            
            assert result == "Response to large context"
            mock_format.assert_called_once()

    def test_query_with_mixed_encoding(self):
        """
        Testa consulta com encoding misto e caracteres internacionais.
        
        Este teste verifica se o engine consegue processar adequadamente
        conte√∫do e perguntas com encoding misto, incluindo caracteres
        especiais, acentos e diferentes sistemas de escrita. √â essencial
        para suporte internacional e multil√≠ngue.
        
        Cen√°rio testado:
        - Documento com texto em espanhol, ingl√™s e chin√™s
        - Pergunta em espanhol com acentos
        - Sistema preserva todos os caracteres corretamente
        - Nenhum problema de encoding ou corrup√ß√£o
        - Suporte robusto para conte√∫do internacional
        """
        engine = RAGQueryEngine(self.mock_llm, self.mock_chroma, self.engine_config)
        
        mock_doc = Mock()
        mock_doc.page_content = "Texto en espa√±ol con a√ßc√©ntos and English mixed ‰∏≠Êñá"
        mock_doc.metadata = {"source": "mixed_encoding.txt"}
        
        self.mock_chroma.similarity_search_with_relevance_scores.return_value = [(mock_doc, 0.8)]
        self.mock_llm.invoke.return_value = "Response with mixed encoding"
        
        unicode_question = "¬øQu√© dice el documento?"
        
        with patch('rag_project.query_data.logger'), \
             patch('rag_project.query_data.format_prompt') as mock_format, \
             patch('rag_project.query_data.validate_query_inputs'):
            
            mock_format.return_value = "formatted_prompt"
            
            result = engine.query(unicode_question)
            
            assert result == "Response with mixed encoding"


class TestCreateRagEngine:
    """Test cases for create_rag_engine function."""

    @patch('rag_project.query_data.load_local_llama')
    @patch('rag_project.query_data.init_chroma')
    @patch('rag_project.query_data.RAGQueryEngine')
    @patch('rag_project.query_data.logger')
    def test_create_rag_engine_success(self, mock_logger, mock_rag_engine, mock_init_chroma, mock_load_llama):
        """
        Testa a cria√ß√£o bem-sucedida de um engine RAG completo.
        
        Este teste verifica se a fun√ß√£o factory consegue orquestrar
        corretamente a inicializa√ß√£o de todos os componentes do
        sistema RAG (LLM, banco vetorial e engine). √â crucial para
        garantir que o sistema seja configur√°vel e reutiliz√°vel.
        
        Cen√°rio testado:
        - LLM √© carregado com configura√ß√£o espec√≠fica
        - Banco vetorial Chroma √© inicializado
        - RAGQueryEngine √© criado com componentes corretos
        - Todas as configura√ß√µes s√£o preservadas
        - Depend√™ncias s√£o injetadas adequadamente
        """
        mock_llm = Mock()
        mock_chroma = Mock()
        mock_engine = Mock()
        
        mock_load_llama.return_value = mock_llm
        mock_init_chroma.return_value = mock_chroma
        mock_rag_engine.return_value = mock_engine
        
        config = LLMConfig(temperature=0.7)
        
        result = create_rag_engine(
            chroma_path="/test/path",
            collection_name="test_collection",
            model="test_model",
            model_type="llama",
            llm_config=config
        )
        
        assert result == mock_engine
        
        mock_load_llama.assert_called_once_with("test_model", "llama", config)
        mock_init_chroma.assert_called_once_with("/test/path", "test_collection")
        
        mock_rag_engine.assert_called_once()
        call_args = mock_rag_engine.call_args
        assert call_args[1]['llm'] == mock_llm
        assert call_args[1]['chroma'] == mock_chroma
        
        engine_config = call_args[1]['engine_config']
        assert engine_config['model'] == "test_model"
        assert engine_config['model_type'] == "llama"
        assert engine_config['chroma_path'] == "/test/path"
        assert engine_config['collection_name'] == "test_collection"

    @patch('rag_project.query_data.load_local_llama')
    @patch('rag_project.query_data.logger')
    def test_create_rag_engine_llm_error(self, mock_logger, mock_load_llama):
        """
        Testa o tratamento de erros durante carregamento do LLM.
        
        Este teste verifica se a fun√ß√£o factory trata adequadamente
        falhas que podem ocorrer durante o carregamento do modelo LLM.
        √â importante para garantir que erros sejam propagados
        corretamente e o sistema falhe de forma controlada.
        
        Cen√°rio testado:
        - Carregamento do LLM falha com RuntimeError
        - Erro √© propagado para o chamador
        - Sistema n√£o tenta continuar com LLM inv√°lido
        - Falha acontece cedo no processo de inicializa√ß√£o
        """
        mock_load_llama.side_effect = RuntimeError("LLM failed")
        
        with pytest.raises(RuntimeError):
            create_rag_engine("/test/path", "test_collection")

    @patch('rag_project.query_data.load_local_llama')
    @patch('rag_project.query_data.init_chroma')
    @patch('rag_project.query_data.logger')
    def test_create_rag_engine_chroma_error(self, mock_logger, mock_init_chroma, mock_load_llama):
        """
        Testa o tratamento de erros durante inicializa√ß√£o do Chroma.
        
        Este teste verifica se a fun√ß√£o factory trata adequadamente
        falhas que podem ocorrer durante a inicializa√ß√£o do banco
        vetorial Chroma. √â importante para garantir que o sistema
        falhe de forma controlada quando componentes cr√≠ticos falham.
        
        Cen√°rio testado:
        - LLM √© carregado com sucesso
        - Inicializa√ß√£o do Chroma falha com RuntimeError
        - Erro √© propagado corretamente para o chamador
        - Sistema n√£o tenta continuar sem banco vetorial
        """
        mock_load_llama.return_value = Mock()
        mock_init_chroma.side_effect = RuntimeError("Chroma failed")
        
        with pytest.raises(RuntimeError):
            create_rag_engine("/test/path", "test_collection")

    @patch('rag_project.query_data.load_local_llama')
    @patch('rag_project.query_data.init_chroma')
    @patch('rag_project.query_data.RAGQueryEngine')
    @patch('rag_project.query_data.logger')
    def test_create_rag_engine_with_defaults(self, mock_logger, mock_rag_engine, mock_init_chroma, mock_load_llama):
        """
        Testa a cria√ß√£o do engine RAG usando apenas par√¢metros obrigat√≥rios.
        
        Este teste verifica se a fun√ß√£o factory consegue criar um engine
        funcional utilizando apenas os par√¢metros m√≠nimos necess√°rios,
        preenchendo o resto com valores padr√£o. √â importante para
        facilitar o uso da biblioteca.
        
        Cen√°rio testado:
        - Apenas caminho do Chroma e nome da cole√ß√£o fornecidos
        - Fun√ß√£o usa valores padr√£o para modelo e configura√ß√µes
        - Engine √© criado com sucesso
        - Todas as fun√ß√µes de inicializa√ß√£o s√£o chamadas
        """
        mock_llm = Mock()
        mock_chroma = Mock()
        mock_engine = Mock()
        
        mock_load_llama.return_value = mock_llm
        mock_init_chroma.return_value = mock_chroma
        mock_rag_engine.return_value = mock_engine
        
        result = create_rag_engine("/test/path", "test_collection")
        
        assert result == mock_engine
        mock_load_llama.assert_called_once()
        mock_init_chroma.assert_called_once()

    def test_create_rag_engine_config_combinations(self):
        """
        Testa diferentes combina√ß√µes de configura√ß√µes para cria√ß√£o do engine.
        
        Este teste verifica se a fun√ß√£o factory consegue lidar com
        diferentes combina√ß√µes de par√¢metros de configura√ß√£o, incluindo
        caminhos longos e nomes complexos. √â importante para garantir
        flexibilidade e robustez da fun√ß√£o.
        
        Cen√°rio testado:
        - M√∫ltiplas combina√ß√µes de caminhos e nomes
        - Caminhos curtos e longos s√£o suportados
        - Nomes de cole√ß√£o simples e complexos funcionam
        - Fun√ß√£o √© robusta para diferentes casos de uso
        """
        test_cases = [
            ("/path1", "collection1", "model1", "type1"),
            ("/path2", "collection2", "model2", "type2"),
            ("/very/long/path/to/test/deep/directory/structure", "long_collection_name", "model", "type"),
        ]
        
        for chroma_path, collection_name, model, model_type in test_cases:
            with patch('rag_project.query_data.load_local_llama') as mock_load_llama, \
                 patch('rag_project.query_data.init_chroma') as mock_init_chroma, \
                 patch('rag_project.query_data.RAGQueryEngine') as mock_rag_engine, \
                 patch('rag_project.query_data.logger'):
                
                mock_load_llama.return_value = Mock()
                mock_init_chroma.return_value = Mock()
                mock_engine = Mock()
                mock_rag_engine.return_value = mock_engine
                
                result = create_rag_engine(chroma_path, collection_name, model, model_type)
                
                assert result == mock_engine
                mock_load_llama.assert_called()
                mock_init_chroma.assert_called_with(chroma_path, collection_name)


class TestLLMConfigVariations:
    """Test different LLMConfig variations."""

    def test_llm_config_variations(self):
        """
        Testa diferentes varia√ß√µes de configura√ß√£o do LLM.
        
        Este teste verifica se o sistema consegue carregar o LLM com
        diferentes configura√ß√µes de par√¢metros, incluindo valores
        extremos (temperatura 0.0 e 1.0, tokens m√≠nimo e m√°ximo).
        √â importante para garantir flexibilidade na configura√ß√£o.
        
        Cen√°rio testado:
        - Configura√ß√£o padr√£o (sem par√¢metros)
        - Temperatura conservadora (0.0) e criativa (1.0)
        - Tokens m√≠nimo (1) e m√°ximo (2048)
        - Sistema aceita todas as varia√ß√µes v√°lidas
        - Nenhuma configura√ß√£o causa falha inesperada
        """
        configs = [
            LLMConfig(),
            LLMConfig(temperature=0.0),
            LLMConfig(temperature=1.0),
            LLMConfig(max_new_tokens=1),
            LLMConfig(max_new_tokens=2048),
        ]
        
        for config in configs:
            with patch('rag_project.query_data.Path') as mock_path, \
                 patch('rag_project.query_data.CTransformers') as mock_ctransformers, \
                 patch('rag_project.query_data.logger'):
                
                mock_path.return_value.exists.return_value = True
                mock_ctransformers.return_value = Mock()
                
                result = load_local_llama("test_model", "llama", config)
                assert result is not None


class TestIntegrationAndPerformance:
    """Integration and performance tests."""

    def test_full_workflow_mocked(self):
        """
        Testa o fluxo completo do sistema RAG de ponta a ponta.
        
        Este √© um teste de integra√ß√£o que verifica se todos os
        componentes do sistema RAG funcionam juntos corretamente,
        desde a inicializa√ß√£o at√© a gera√ß√£o da resposta final.
        √â essencial para validar que mudan√ßas em componentes
        individuais n√£o quebrem o sistema como um todo.
        
        Cen√°rio testado:
        - Engine RAG √© criado via factory function
        - Documento √© recuperado do banco vetorial
        - Prompt √© formatado corretamente
        - LLM gera resposta baseada no contexto
        - Pipeline completo funciona sem erros
        """
        with patch('rag_project.query_data.load_local_llama') as mock_load_llama, \
             patch('rag_project.query_data.init_chroma') as mock_init_chroma, \
             patch('rag_project.query_data.logger'):
            
            mock_llm = Mock()
            mock_chroma = Mock()
            mock_load_llama.return_value = mock_llm
            mock_init_chroma.return_value = mock_chroma
            
            mock_doc = Mock()
            mock_doc.page_content = "Test document content"
            mock_doc.metadata = {"source": "test.txt"}
            mock_chroma.similarity_search_with_relevance_scores.return_value = [(mock_doc, 0.8)]
            mock_llm.invoke.return_value = "Test response"
            
            engine = create_rag_engine("/test/path", "test_collection")
            
            with patch('rag_project.query_data.validate_query_inputs'), \
                 patch('rag_project.query_data.format_prompt') as mock_format:
                mock_format.return_value = "formatted prompt"
                
                result = engine.query("test question")
                
                assert result == "Test response"

    @patch('rag_project.query_data.logger')
    def test_large_context_handling(self, mock_logger):
        """
        Testa o manuseio de documentos com contexto muito grande.
        
        Este teste verifica se o sistema consegue processar documentos
        com conte√∫do extenso sem problemas de performance ou mem√≥ria.
        √â importante para garantir que o sistema seja escal√°vel e
        possa trabalhar com documentos reais que podem ser longos.
        
        Cen√°rio testado:
        - Documento com 10.000 caracteres √© processado
        - Sistema n√£o falha com contexto grande
        - Mem√≥ria √© gerenciada adequadamente
        - Resposta √© gerada normalmente
        - Performance se mant√©m aceit√°vel
        """
        mock_llm = Mock()
        mock_chroma = Mock()
        engine = RAGQueryEngine(mock_llm, mock_chroma, {
            "model": "test", "model_type": "test", "chroma_path": "test",
            "collection_name": "test", "llm_config": {}
        })
        
        large_context = "A" * 10000
        mock_doc = Mock()
        mock_doc.page_content = large_context
        
        mock_chroma.similarity_search_with_relevance_scores.return_value = [(mock_doc, 0.8)]
        mock_llm.invoke.return_value = "Response"
        
        with patch('rag_project.query_data.format_prompt') as mock_format, \
             patch('rag_project.query_data.validate_query_inputs'):
            mock_format.return_value = "formatted"
            
            result = engine.query("test")
            assert result == "Response"

    @patch('rag_project.query_data.logger')
    def test_many_documents_retrieval(self, mock_logger):
        """
        Testa a recupera√ß√£o de um grande n√∫mero de documentos.
        
        Este teste verifica se o sistema consegue processar consultas
        que retornam muitos documentos relevantes. √â importante para
        casos onde h√° muito conte√∫do similar ou quando se deseja
        contexto mais abrangente para o LLM.
        
        Cen√°rio testado:
        - 100 documentos s√£o retornados pelo Chroma
        - Sistema processa todos sem falhar
        - Lista de documentos mant√©m integridade
        - Performance com volume alto √© aceit√°vel
        - Mem√≥ria √© gerenciada adequadamente
        """
        mock_llm = Mock()
        mock_chroma = Mock()
        engine = RAGQueryEngine(mock_llm, mock_chroma, {
            "model": "test", "model_type": "test", "chroma_path": "test",
            "collection_name": "test", "llm_config": {}
        })
        
        mock_docs = [Mock() for _ in range(100)]
        mock_results = [(doc, 0.8) for doc in mock_docs]
        
        mock_chroma.similarity_search_with_relevance_scores.return_value = mock_results
        
        result = engine.retrieve_documents("test", documents_retrieve=100)
        assert len(result) == 100


class TestLoggingAndMonitoring:
    """Test logging and monitoring functionality."""

    def test_all_functions_log_execution_time(self):
        """
        Testa se todas as fun√ß√µes principais fazem logging adequado.
        
        Este teste verifica se cada fun√ß√£o do sistema registra informa√ß√µes
        suficientes nos logs para debugging e monitoramento. √â essencial
        para troubleshooting em produ√ß√£o e an√°lise de performance.
        
        Cen√°rio testado:
        - Cada fun√ß√£o principal √© executada
        - Logs s√£o gerados (info, error, ou debug)
        - Sistema √© observ√°vel e audit√°vel
        - Debugging √© facilitado por logging adequado
        - Monitoramento pode ser implementado baseado nos logs
        """
        functions_to_test = [
            ('load_local_llama', load_local_llama),
            ('init_chroma', init_chroma),
            ('format_prompt', format_prompt),
            ('validate_query_inputs', validate_query_inputs),
            ('create_rag_engine', create_rag_engine),
        ]
        
        for func_name, func in functions_to_test:
            with patch('rag_project.query_data.logger') as mock_logger:
                try:
                    if func_name == 'load_local_llama':
                        with patch('rag_project.query_data.Path') as mock_path, \
                             patch('rag_project.query_data.CTransformers'):
                            mock_path.return_value.exists.return_value = False
                            func()
                    elif func_name == 'init_chroma':
                        with patch('rag_project.query_data.init_embeddings'):
                            func("/test", "test")
                    elif func_name == 'format_prompt':
                        func("context", "question")
                    elif func_name == 'validate_query_inputs':
                        func("valid question")
                    elif func_name == 'create_rag_engine':
                        with patch('rag_project.query_data.load_local_llama'), \
                             patch('rag_project.query_data.init_chroma'), \
                             patch('rag_project.query_data.RAGQueryEngine'):
                            func("/test", "test")
                except Exception:
                    pass
                
                assert mock_logger.info.called or mock_logger.error.called or mock_logger.debug.called


if __name__ == "__main__":
    pytest.main([__file__, "-v"])