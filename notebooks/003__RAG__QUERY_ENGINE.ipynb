{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c983c93",
   "metadata": {},
   "source": [
    "# RAG Query Engine Testing\n",
    "\n",
    "This notebook tests the `RAGQueryEngine` class to ensure it produces similar results to the manual RAG implementation in notebook 002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5013b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lealdx/personal/rag_chat_project/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from rag_project.constants import (\n",
    "    BOOKS_CHROMA_DB_DIR, \n",
    "    BOOKS_COLLECTION_NAME\n",
    ")\n",
    "from rag_project.query_data import create_rag_engine, RAGQueryEngine\n",
    "from rag_project.rag_models import LLMConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35eb144",
   "metadata": {},
   "source": [
    "# Initialize RAG Query Engine\n",
    "\n",
    "Create and configure the RAG Query Engine using the factory function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a05ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: create_rag_engine called with parameters: chroma_path='/home/lealdx/personal/rag_chat_project/data/chroma-db/books', collection_name='books', model='TheBloke/Llama-2-7B-Chat-GGUF', model_type='llama', llm_config={'max_new_tokens': 256, 'temperature': 0.3, 'top_k': 40, 'top_p': 0.9, 'repetition_penalty': 1.05, 'stop': ['</s>', '[/INST]'], 'context_length': 2048, 'threads': 4, 'seed': 42, 'stream': True}\n",
      "INFO: Initializing LLM component\n",
      "INFO: load_local_llama called with parameters: model='TheBloke/Llama-2-7B-Chat-GGUF', model_type='llama', config={'max_new_tokens': 256, 'temperature': 0.3, 'top_k': 40, 'top_p': 0.9, 'repetition_penalty': 1.05, 'stop': ['</s>', '[/INST]'], 'context_length': 2048, 'threads': 4, 'seed': 42, 'stream': True}\n",
      "INFO: Checking model file existence at path: /home/lealdx/personal/rag_chat_project/models/llama-2-7b-chat.Q4_K_M.gguf\n",
      "INFO: Initializing CTransformers with model file: /home/lealdx/personal/rag_chat_project/models/llama-2-7b-chat.Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 5482.75it/s]\n",
      "Fetching 0 files: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: load_local_llama completed successfully in 3.031 seconds. LLM type: CTransformers, Model: TheBloke/Llama-2-7B-Chat-GGUF\n",
      "INFO: Initializing Chroma DB component\n",
      "INFO: init_chroma called with parameters: chroma_str_path='/home/lealdx/personal/rag_chat_project/data/chroma-db/books', collection_name='books'\n",
      "INFO: Initializing embeddings function\n",
      "INFO: Embeddings function initialized successfully\n",
      "INFO: Checking Chroma directory path: /home/lealdx/personal/rag_chat_project/data/chroma-db/books\n",
      "INFO: Initializing Chroma DB at /home/lealdx/personal/rag_chat_project/data/chroma-db/books with collection 'books'\n",
      "INFO: init_chroma completed successfully in 4.220 seconds. Collection: books, Path: /home/lealdx/personal/rag_chat_project/data/chroma-db/books, DB type: Chroma\n",
      "INFO: Building engine configuration\n",
      "INFO: Creating RAGQueryEngine instance\n",
      "INFO: RAGQueryEngine.__init__ called with parameters: llm_type='CTransformers', chroma_type='Chroma', engine_config_keys=['model', 'model_type', 'chroma_path', 'collection_name', 'llm_config']\n",
      "INFO: RAG Query Engine initialized successfully in 0.001 seconds. Model: TheBloke/Llama-2-7B-Chat-GGUF, Collection: books, Path: /home/lealdx/personal/rag_chat_project/data/chroma-db/books\n",
      "INFO: create_rag_engine completed successfully in 7.263 seconds. Engine: RAGQueryEngine, Model: TheBloke/Llama-2-7B-Chat-GGUF, Collection: books\n",
      "RAG Query Engine initialized: RAGQueryEngine(model='TheBloke/Llama-2-7B-Chat-GGUF', collection='books')\n",
      "INFO: get_config_info called\n",
      "INFO: get_config_info completed successfully in 0.001 seconds. Config keys: ['model', 'model_type', 'chroma_path', 'collection_name', 'llm_config']\n",
      "Configuration: {'model': 'TheBloke/Llama-2-7B-Chat-GGUF', 'model_type': 'llama', 'chroma_path': '/home/lealdx/personal/rag_chat_project/data/chroma-db/books', 'collection_name': 'books', 'llm_config': {'max_new_tokens': 256, 'temperature': 0.3, 'top_k': 40, 'top_p': 0.9, 'repetition_penalty': 1.05, 'stop': ['</s>', '[/INST]'], 'context_length': 2048, 'threads': 4, 'seed': 42, 'stream': True}}\n"
     ]
    }
   ],
   "source": [
    "# Create LLM configuration matching the manual implementation\n",
    "llm_config = LLMConfig()\n",
    "\n",
    "# Initialize RAG Query Engine\n",
    "engine = create_rag_engine(\n",
    "    chroma_path=BOOKS_CHROMA_DB_DIR,\n",
    "    collection_name=BOOKS_COLLECTION_NAME,\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "print(f\"RAG Query Engine initialized: {engine}\")\n",
    "print(f\"Configuration: {engine.get_config_info()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd666e",
   "metadata": {},
   "source": [
    "# Test Queries\n",
    "\n",
    "Define the same test query used in notebook 002 for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c601591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary test query: Who is the White Rabbit and how does Alice first meet him?\n",
      "Additional test queries: 5 queries available\n"
     ]
    }
   ],
   "source": [
    "# Same query used in notebook 002 for comparison\n",
    "query_text = \"Who is the White Rabbit and how does Alice first meet him?\"\n",
    "\n",
    "# Additional test queries from notebook 002 comments\n",
    "specific_queries = [\n",
    "    \"What does Alice drink or eat that makes her change size?\",\n",
    "    \"Describe the Duchess's kitchen and what happens there\",\n",
    "    \"What games are played in Wonderland?\",\n",
    "    \"What poems or songs are recited in the story?\",\n",
    "    \"How does Alice's adventure end?\"\n",
    "]\n",
    "\n",
    "print(f\"Primary test query: {query_text}\")\n",
    "print(f\"Additional test queries: {len(specific_queries)} queries available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2ae87",
   "metadata": {},
   "source": [
    "# Test 1: Basic Query Method\n",
    "\n",
    "Test the basic `query()` method which returns just the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0648e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Basic Query Test ===\n",
      "INFO: query called with parameters: question_length=58, documents_retrieve=3, max_length=500, min_similarity_score=0.500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Validating query inputs\n",
      "INFO: validate_query_inputs called with parameters: question_length=58, max_length=500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: validate_query_inputs completed successfully in 0.003 seconds. Question validated: length=58\n",
      "INFO: Processing query: Who is the White Rabbit and how does Alice first m...\n",
      "INFO: Retrieving documents\n",
      "INFO: retrieve_documents called with parameters: question_length=58, documents_retrieve=3, min_similarity_score=0.500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Performing similarity search with Chroma DB\n",
      "INFO: Filtering documents by similarity score threshold: 0.500\n",
      "INFO: retrieve_documents completed successfully in 0.143 seconds. Retrieved 3 documents (filtered from 3 total) for query\n",
      "INFO: Document similarity scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401]\n",
      "INFO: Building context from 3 retrieved documents\n",
      "INFO: Formatting prompt with template\n",
      "INFO: format_prompt called with parameters: context_length=899, question_length=58, template_length=357, context_preview='So Alice began telling them her adventures from the time when she first saw the White Rabbit. She wa...', question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: format_prompt completed successfully in 0.001 seconds. Output length: 1295 characters\n",
      "INFO: Generating answer with LLM\n",
      "INFO: generate_answer called with parameters: prompt_length=1295, prompt_preview='\n",
      "[INST] <<SYS>>\n",
      "You are a clear and concise assistant specialized in summarization and explanation.\n",
      "Use only the provided context to answer the question.\n",
      "If the context does not provide enough informa...'\n",
      "INFO: Invoking LLM for answer generation\n",
      "INFO: generate_answer completed successfully in 100.502 seconds. Answer length: 263, answer_preview='Based on the provided context, the answer to your question is:\n",
      "The White Rabbit is a character who a...'\n",
      "INFO: query completed successfully in 100.669 seconds. Answer length: 263, documents_used: 3, question: Who is the White Rabbit and how does Alice first m...\n",
      "Question: Who is the White Rabbit and how does Alice first meet him?\n",
      "Answer: Based on the provided context, the answer to your question is:\n",
      "The White Rabbit is a character who appears in Alice's adventures. Alice first meets him when he is talking to himself in a hurry, smiling at everything that is said, and goes by without noticing her.\n",
      "Answer length: 263 characters\n"
     ]
    }
   ],
   "source": [
    "# Test basic query method\n",
    "print(\"=== Basic Query Test ===\")\n",
    "basic_answer = engine.query(query_text)\n",
    "print(f\"Question: {query_text}\")\n",
    "print(f\"Answer: {basic_answer}\")\n",
    "print(f\"Answer length: {len(basic_answer)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a7a84",
   "metadata": {},
   "source": [
    "# Test 2: Query with Metadata\n",
    "\n",
    "Test the `query_with_metadata()` method which returns complete information like in notebook 002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f6e549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query with Metadata Test ===\n",
      "INFO: query_with_metadata called with parameters: question_length=58, documents_retrieve=3, max_length=500, min_similarity_score=0.500, return_sources=True, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Validating query inputs for metadata query\n",
      "INFO: validate_query_inputs called with parameters: question_length=58, max_length=500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: validate_query_inputs completed successfully in 0.002 seconds. Question validated: length=58\n",
      "INFO: Processing query with metadata: Who is the White Rabbit and how does Alice first m...\n",
      "INFO: Retrieving documents with scores\n",
      "INFO: retrieve_documents_with_scores called with parameters: question_length=58, documents_retrieve=3, min_similarity_score=0.500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Performing similarity search with Chroma DB (with scores)\n",
      "INFO: Filtering documents and scores by similarity threshold: 0.500\n",
      "INFO: retrieve_documents_with_scores completed successfully in 0.060 seconds. Retrieved 3 documents (filtered from 3 total) with scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401]\n",
      "INFO: Building context from 3 retrieved documents\n",
      "INFO: Formatting prompt with template for metadata query\n",
      "INFO: format_prompt called with parameters: context_length=899, question_length=58, template_length=357, context_preview='So Alice began telling them her adventures from the time when she first saw the White Rabbit. She wa...', question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: format_prompt completed successfully in 0.001 seconds. Output length: 1295 characters\n",
      "INFO: Generating answer with LLM for metadata query\n",
      "INFO: generate_answer called with parameters: prompt_length=1295, prompt_preview='\n",
      "[INST] <<SYS>>\n",
      "You are a clear and concise assistant specialized in summarization and explanation.\n",
      "Use only the provided context to answer the question.\n",
      "If the context does not provide enough informa...'\n",
      "INFO: Invoking LLM for answer generation\n",
      "INFO: generate_answer completed successfully in 23.244 seconds. Answer length: 263, answer_preview='Based on the provided context, the answer to your question is:\n",
      "The White Rabbit is a character who a...'\n",
      "INFO: Extracting sources from document metadata\n",
      "INFO: Extracted 3 sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n",
      "INFO: query_with_metadata completed successfully in 23.330 seconds. Answer length: 263, documents_used: 3, sources: 3, similarity_scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401], question: Who is the White Rabbit and how does Alice first m...\n",
      "Question: Who is the White Rabbit and how does Alice first meet him?\n",
      "Answer: Based on the provided context, the answer to your question is:\n",
      "The White Rabbit is a character who appears in Alice's adventures. Alice first meets him when he is talking to himself in a hurry, smiling at everything that is said, and goes by without noticing her.\n",
      "Sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n",
      "Retrieved documents: 3\n",
      "Similarity scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401]\n",
      "\n",
      "=== Formatted Response (like notebook 002) ===\n",
      "Response: Based on the provided context, the answer to your question is:\n",
      "The White Rabbit is a character who appears in Alice's adventures. Alice first meets him when he is talking to himself in a hurry, smiling at everything that is said, and goes by without noticing her.\n",
      "Sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n"
     ]
    }
   ],
   "source": [
    "# Test query with metadata (similar to notebook 002 results)\n",
    "print(\"=== Query with Metadata Test ===\")\n",
    "metadata_response = engine.query_with_metadata(query_text)\n",
    "\n",
    "print(f\"Question: {metadata_response.query}\")\n",
    "print(f\"Answer: {metadata_response.answer}\")\n",
    "print(f\"Sources: {metadata_response.sources}\")\n",
    "print(f\"Retrieved documents: {metadata_response.retrieved_docs}\")\n",
    "print(f\"Similarity scores: {metadata_response.similarity_scores}\")\n",
    "\n",
    "# Format similar to notebook 002 output\n",
    "formatted_response = f\"Response: {metadata_response.answer}\\nSources: {metadata_response.sources}\"\n",
    "print(\"\\n=== Formatted Response (like notebook 002) ===\")\n",
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4b7ce",
   "metadata": {},
   "source": [
    "# Test 3: Document Retrieval Comparison\n",
    "\n",
    "Test the document retrieval functionality to compare with notebook 002 manual retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0c2f025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Document Retrieval Test ===\n",
      "INFO: retrieve_documents_with_scores called with parameters: question_length=58, documents_retrieve=3, min_similarity_score=0.500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Performing similarity search with Chroma DB (with scores)\n",
      "INFO: Filtering documents and scores by similarity threshold: 0.500\n",
      "INFO: retrieve_documents_with_scores completed successfully in 0.040 seconds. Retrieved 3 documents (filtered from 3 total) with scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401]\n",
      "Query: Who is the White Rabbit and how does Alice first meet him?\n",
      "Found 3 relevant documents\n",
      "Retrieved 3 documents with scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401]\n",
      "\n",
      "=== Retrieved Documents Preview ===\n",
      "Document 1 (score: 0.7235):\n",
      "  Content preview: So Alice began telling them her adventures from the time when she first saw the White Rabbit. She wa...\n",
      "  Source: /home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md\n",
      "\n",
      "Document 2 (score: 0.6556):\n",
      "  Content preview: they were all ornamented with hearts. Next came the guests, mostly Kings and Queens, and among them ...\n",
      "  Source: /home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md\n",
      "\n",
      "Document 3 (score: 0.6501):\n",
      "  Content preview: Alice watched the White Rabbit as he fumbled over the list, feeling very curious to see what the nex...\n",
      "  Source: /home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test document retrieval with scores (similar to notebook 002)\n",
    "print(\"=== Document Retrieval Test ===\")\n",
    "docs, scores = engine.retrieve_documents_with_scores(query_text)\n",
    "\n",
    "print(f\"Query: {query_text}\")\n",
    "print(f\"Found {len(docs)} relevant documents\")\n",
    "\n",
    "if len(docs) == 0:\n",
    "    print(\"Unable to find matching results.\")\n",
    "else:\n",
    "    print(f\"Retrieved {len(docs)} documents with scores: {scores}\")\n",
    "    \n",
    "    # Show first few characters of each document (like notebook 002 would show)\n",
    "    print(\"\\n=== Retrieved Documents Preview ===\")\n",
    "    for i, (doc, score) in enumerate(zip(docs, scores)):\n",
    "        print(f\"Document {i+1} (score: {score:.4f}):\")\n",
    "        print(f\"  Content preview: {doc.page_content[:100]}...\")\n",
    "        print(f\"  Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e30d4c0",
   "metadata": {},
   "source": [
    "# Test 4: Multiple Queries Comparison\n",
    "\n",
    "Test multiple queries to ensure consistent performance across different questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b37917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multiple Queries Test ===\n",
      "\n",
      "--- Query 1 ---\n",
      "Question: What does Alice drink or eat that makes her change size?\n",
      "INFO: query_with_metadata called with parameters: question_length=56, documents_retrieve=3, max_length=500, min_similarity_score=0.500, return_sources=True, question_preview='What does Alice drink or eat that makes her change size?'\n",
      "INFO: Validating query inputs for metadata query\n",
      "INFO: validate_query_inputs called with parameters: question_length=56, max_length=500, question_preview='What does Alice drink or eat that makes her change size?'\n",
      "INFO: validate_query_inputs completed successfully in 0.001 seconds. Question validated: length=56\n",
      "INFO: Processing query with metadata: What does Alice drink or eat that makes her change...\n",
      "INFO: Retrieving documents with scores\n",
      "INFO: retrieve_documents_with_scores called with parameters: question_length=56, documents_retrieve=3, min_similarity_score=0.500, question_preview='What does Alice drink or eat that makes her change size?'\n",
      "INFO: Performing similarity search with Chroma DB (with scores)\n",
      "INFO: Filtering documents and scores by similarity threshold: 0.500\n",
      "INFO: retrieve_documents_with_scores completed successfully in 0.032 seconds. Retrieved 2 documents (filtered from 3 total) with scores: [0.5332419023272539, 0.5181275788359136]\n",
      "INFO: Building context from 2 retrieved documents\n",
      "INFO: Formatting prompt with template for metadata query\n",
      "INFO: format_prompt called with parameters: context_length=595, question_length=56, template_length=357, context_preview='The great question certainly was, what? Alice looked all round her at the flowers and the blades of ...', question_preview='What does Alice drink or eat that makes her change size?'\n",
      "INFO: format_prompt completed successfully in 0.001 seconds. Output length: 989 characters\n",
      "INFO: Generating answer with LLM for metadata query\n",
      "INFO: generate_answer called with parameters: prompt_length=989, prompt_preview='\n",
      "[INST] <<SYS>>\n",
      "You are a clear and concise assistant specialized in summarization and explanation.\n",
      "Use only the provided context to answer the question.\n",
      "If the context does not provide enough informa...'\n",
      "INFO: Invoking LLM for answer generation\n",
      "INFO: generate_answer completed successfully in 58.961 seconds. Answer length: 269, answer_preview='The context does not contain enough information to answer. The passage only describes Alice's confus...'\n",
      "INFO: Extracting sources from document metadata\n",
      "INFO: Extracted 2 sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n",
      "INFO: query_with_metadata completed successfully in 59.019 seconds. Answer length: 269, documents_used: 2, sources: 2, similarity_scores: [0.5332419023272539, 0.5181275788359136], question: What does Alice drink or eat that makes her change...\n",
      "Answer: The context does not contain enough information to answer. The passage only describes Alice's confusion and frustration as she tries to determine what to eat or drink, but it does not provide any info...\n",
      "Retrieved docs: 2\n",
      "Sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Query 2 ---\n",
      "Question: Describe the Duchess's kitchen and what happens there\n",
      "INFO: query_with_metadata called with parameters: question_length=53, documents_retrieve=3, max_length=500, min_similarity_score=0.500, return_sources=True, question_preview='Describe the Duchess's kitchen and what happens there'\n",
      "INFO: Validating query inputs for metadata query\n",
      "INFO: validate_query_inputs called with parameters: question_length=53, max_length=500, question_preview='Describe the Duchess's kitchen and what happens there'\n",
      "INFO: validate_query_inputs completed successfully in 0.001 seconds. Question validated: length=53\n",
      "INFO: Processing query with metadata: Describe the Duchess's kitchen and what happens th...\n",
      "INFO: Retrieving documents with scores\n",
      "INFO: retrieve_documents_with_scores called with parameters: question_length=53, documents_retrieve=3, min_similarity_score=0.500, question_preview='Describe the Duchess's kitchen and what happens there'\n",
      "INFO: Performing similarity search with Chroma DB (with scores)\n",
      "INFO: Filtering documents and scores by similarity threshold: 0.500\n",
      "INFO: retrieve_documents_with_scores completed successfully in 0.023 seconds. Retrieved 1 documents (filtered from 3 total) with scores: [0.5792379803657993]\n",
      "INFO: Building context from 1 retrieved documents\n",
      "INFO: Formatting prompt with template for metadata query\n",
      "INFO: format_prompt called with parameters: context_length=265, question_length=53, template_length=357, context_preview='The door led right into a large kitchen, which was full of smoke from one end to the other: the Duch...', question_preview='Describe the Duchess's kitchen and what happens there'\n",
      "INFO: format_prompt completed successfully in 0.001 seconds. Output length: 656 characters\n",
      "INFO: Generating answer with LLM for metadata query\n",
      "INFO: generate_answer called with parameters: prompt_length=656, prompt_preview='\n",
      "[INST] <<SYS>>\n",
      "You are a clear and concise assistant specialized in summarization and explanation.\n",
      "Use only the provided context to answer the question.\n",
      "If the context does not provide enough informa...'\n",
      "INFO: Invoking LLM for answer generation\n",
      "INFO: generate_answer completed successfully in 48.291 seconds. Answer length: 299, answer_preview='Based on the provided context, the Duchess's kitchen is a large space with a lot of smoke filling it...'\n",
      "INFO: Extracting sources from document metadata\n",
      "INFO: Extracted 1 sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n",
      "INFO: query_with_metadata completed successfully in 48.333 seconds. Answer length: 299, documents_used: 1, sources: 1, similarity_scores: [0.5792379803657993], question: Describe the Duchess's kitchen and what happens th...\n",
      "Answer: Based on the provided context, the Duchess's kitchen is a large space with a lot of smoke filling it from one end to the other. The Duchess is sitting on a three-legged stool in the middle, nursing a ...\n",
      "Retrieved docs: 1\n",
      "Sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Query 3 ---\n",
      "Question: What games are played in Wonderland?\n",
      "INFO: query_with_metadata called with parameters: question_length=36, documents_retrieve=3, max_length=500, min_similarity_score=0.500, return_sources=True, question_preview='What games are played in Wonderland?'\n",
      "INFO: Validating query inputs for metadata query\n",
      "INFO: validate_query_inputs called with parameters: question_length=36, max_length=500, question_preview='What games are played in Wonderland?'\n",
      "INFO: validate_query_inputs completed successfully in 0.001 seconds. Question validated: length=36\n",
      "INFO: Processing query with metadata: What games are played in Wonderland?...\n",
      "INFO: Retrieving documents with scores\n",
      "INFO: retrieve_documents_with_scores called with parameters: question_length=36, documents_retrieve=3, min_similarity_score=0.500, question_preview='What games are played in Wonderland?'\n",
      "INFO: Performing similarity search with Chroma DB (with scores)\n",
      "INFO: Filtering documents and scores by similarity threshold: 0.500\n",
      "INFO: retrieve_documents_with_scores completed successfully in 0.021 seconds. Retrieved 0 documents (filtered from 3 total) with scores: []\n",
      "WARNING: No relevant documents retrieved for metadata query after 0.031 seconds. Returning fallback response\n",
      "Answer: I couldn't find any relevant information to answer your question.\n",
      "Retrieved docs: 0\n",
      "Sources: []\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test multiple queries from the test set\n",
    "print(\"=== Multiple Queries Test ===\")\n",
    "\n",
    "# Test a few queries from the specific_queries list\n",
    "test_queries = specific_queries[:3]  # Test first 3 queries\n",
    "\n",
    "for i, test_query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n--- Query {i} ---\")\n",
    "    print(f\"Question: {test_query}\")\n",
    "    \n",
    "    try:\n",
    "        response = engine.query_with_metadata(test_query)\n",
    "        print(f\"Answer: {response.answer[:200]}...\" if len(response.answer) > 200 else f\"Answer: {response.answer}\")\n",
    "        print(f\"Retrieved docs: {response.retrieved_docs}\")\n",
    "        print(f\"Sources: {response.sources}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {e}\")\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc5af4c",
   "metadata": {},
   "source": [
    "# Test 5: Configuration and Performance\n",
    "\n",
    "Test different configurations and evaluate performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0a164c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Configuration and Performance Test ===\n",
      "\n",
      "--- Testing different document retrieval counts ---\n",
      "INFO: query_with_metadata called with parameters: question_length=58, documents_retrieve=1, max_length=500, min_similarity_score=0.500, return_sources=True, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Validating query inputs for metadata query\n",
      "INFO: validate_query_inputs called with parameters: question_length=58, max_length=500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: validate_query_inputs completed successfully in 0.001 seconds. Question validated: length=58\n",
      "INFO: Processing query with metadata: Who is the White Rabbit and how does Alice first m...\n",
      "INFO: Retrieving documents with scores\n",
      "INFO: retrieve_documents_with_scores called with parameters: question_length=58, documents_retrieve=1, min_similarity_score=0.500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Performing similarity search with Chroma DB (with scores)\n",
      "INFO: Filtering documents and scores by similarity threshold: 0.500\n",
      "INFO: retrieve_documents_with_scores completed successfully in 0.025 seconds. Retrieved 1 documents (filtered from 1 total) with scores: [0.7235418775839446]\n",
      "INFO: Building context from 1 retrieved documents\n",
      "INFO: Formatting prompt with template for metadata query\n",
      "INFO: format_prompt called with parameters: context_length=299, question_length=58, template_length=357, context_preview='So Alice began telling them her adventures from the time when she first saw the White Rabbit. She wa...', question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: format_prompt completed successfully in 0.001 seconds. Output length: 695 characters\n",
      "INFO: Generating answer with LLM for metadata query\n",
      "INFO: generate_answer called with parameters: prompt_length=695, prompt_preview='\n",
      "[INST] <<SYS>>\n",
      "You are a clear and concise assistant specialized in summarization and explanation.\n",
      "Use only the provided context to answer the question.\n",
      "If the context does not provide enough informa...'\n",
      "INFO: Invoking LLM for answer generation\n",
      "INFO: generate_answer completed successfully in 45.692 seconds. Answer length: 332, answer_preview='The context provides enough information to answer your question. According to the passage, the White...'\n",
      "INFO: Extracting sources from document metadata\n",
      "INFO: Extracted 1 sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n",
      "INFO: query_with_metadata completed successfully in 45.742 seconds. Answer length: 332, documents_used: 1, sources: 1, similarity_scores: [0.7235418775839446], question: Who is the White Rabbit and how does Alice first m...\n",
      "Documents retrieved: 1\n",
      "Response time: 45.74 seconds\n",
      "Answer length: 332 characters\n",
      "Actual docs found: 1\n",
      "\n",
      "INFO: query_with_metadata called with parameters: question_length=58, documents_retrieve=3, max_length=500, min_similarity_score=0.500, return_sources=True, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Validating query inputs for metadata query\n",
      "INFO: validate_query_inputs called with parameters: question_length=58, max_length=500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: validate_query_inputs completed successfully in 0.001 seconds. Question validated: length=58\n",
      "INFO: Processing query with metadata: Who is the White Rabbit and how does Alice first m...\n",
      "INFO: Retrieving documents with scores\n",
      "INFO: retrieve_documents_with_scores called with parameters: question_length=58, documents_retrieve=3, min_similarity_score=0.500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Performing similarity search with Chroma DB (with scores)\n",
      "INFO: Filtering documents and scores by similarity threshold: 0.500\n",
      "INFO: retrieve_documents_with_scores completed successfully in 0.030 seconds. Retrieved 3 documents (filtered from 3 total) with scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401]\n",
      "INFO: Building context from 3 retrieved documents\n",
      "INFO: Formatting prompt with template for metadata query\n",
      "INFO: format_prompt called with parameters: context_length=899, question_length=58, template_length=357, context_preview='So Alice began telling them her adventures from the time when she first saw the White Rabbit. She wa...', question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: format_prompt completed successfully in 0.001 seconds. Output length: 1295 characters\n",
      "INFO: Generating answer with LLM for metadata query\n",
      "INFO: generate_answer called with parameters: prompt_length=1295, prompt_preview='\n",
      "[INST] <<SYS>>\n",
      "You are a clear and concise assistant specialized in summarization and explanation.\n",
      "Use only the provided context to answer the question.\n",
      "If the context does not provide enough informa...'\n",
      "INFO: Invoking LLM for answer generation\n",
      "INFO: generate_answer completed successfully in 66.339 seconds. Answer length: 263, answer_preview='Based on the provided context, the answer to your question is:\n",
      "The White Rabbit is a character who a...'\n",
      "INFO: Extracting sources from document metadata\n",
      "INFO: Extracted 3 sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n",
      "INFO: query_with_metadata completed successfully in 66.386 seconds. Answer length: 263, documents_used: 3, sources: 3, similarity_scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401], question: Who is the White Rabbit and how does Alice first m...\n",
      "Documents retrieved: 3\n",
      "Response time: 66.39 seconds\n",
      "Answer length: 263 characters\n",
      "Actual docs found: 3\n",
      "\n",
      "INFO: query_with_metadata called with parameters: question_length=58, documents_retrieve=5, max_length=500, min_similarity_score=0.500, return_sources=True, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Validating query inputs for metadata query\n",
      "INFO: validate_query_inputs called with parameters: question_length=58, max_length=500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: validate_query_inputs completed successfully in 0.001 seconds. Question validated: length=58\n",
      "INFO: Processing query with metadata: Who is the White Rabbit and how does Alice first m...\n",
      "INFO: Retrieving documents with scores\n",
      "INFO: retrieve_documents_with_scores called with parameters: question_length=58, documents_retrieve=5, min_similarity_score=0.500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Performing similarity search with Chroma DB (with scores)\n",
      "INFO: Filtering documents and scores by similarity threshold: 0.500\n",
      "INFO: retrieve_documents_with_scores completed successfully in 0.026 seconds. Retrieved 5 documents (filtered from 5 total) with scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401, 0.5431573277610783, 0.5409084140713855]\n",
      "INFO: Building context from 5 retrieved documents\n",
      "INFO: Formatting prompt with template for metadata query\n",
      "INFO: format_prompt called with parameters: context_length=1500, question_length=58, template_length=357, context_preview='So Alice began telling them her adventures from the time when she first saw the White Rabbit. She wa...', question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: format_prompt completed successfully in 0.001 seconds. Output length: 1896 characters\n",
      "INFO: Generating answer with LLM for metadata query\n",
      "INFO: generate_answer called with parameters: prompt_length=1896, prompt_preview='\n",
      "[INST] <<SYS>>\n",
      "You are a clear and concise assistant specialized in summarization and explanation.\n",
      "Use only the provided context to answer the question.\n",
      "If the context does not provide enough informa...'\n",
      "INFO: Invoking LLM for answer generation\n",
      "INFO: generate_answer completed successfully in 71.138 seconds. Answer length: 323, answer_preview='Based on the provided context, Alice first meets the White Rabbit when he is talking in a hurry and ...'\n",
      "INFO: Extracting sources from document metadata\n",
      "INFO: Extracted 5 sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n",
      "INFO: query_with_metadata completed successfully in 71.181 seconds. Answer length: 323, documents_used: 5, sources: 5, similarity_scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401, 0.5431573277610783, 0.5409084140713855], question: Who is the White Rabbit and how does Alice first m...\n",
      "Documents retrieved: 5\n",
      "Response time: 71.18 seconds\n",
      "Answer length: 323 characters\n",
      "Actual docs found: 5\n",
      "\n",
      "--- Testing different similarity thresholds ---\n",
      "INFO: query_with_metadata called with parameters: question_length=58, documents_retrieve=3, max_length=500, min_similarity_score=0.300, return_sources=True, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Validating query inputs for metadata query\n",
      "INFO: validate_query_inputs called with parameters: question_length=58, max_length=500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: validate_query_inputs completed successfully in 0.001 seconds. Question validated: length=58\n",
      "INFO: Processing query with metadata: Who is the White Rabbit and how does Alice first m...\n",
      "INFO: Retrieving documents with scores\n",
      "INFO: retrieve_documents_with_scores called with parameters: question_length=58, documents_retrieve=3, min_similarity_score=0.300, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Performing similarity search with Chroma DB (with scores)\n",
      "INFO: Filtering documents and scores by similarity threshold: 0.300\n",
      "INFO: retrieve_documents_with_scores completed successfully in 0.075 seconds. Retrieved 3 documents (filtered from 3 total) with scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401]\n",
      "INFO: Building context from 3 retrieved documents\n",
      "INFO: Formatting prompt with template for metadata query\n",
      "INFO: format_prompt called with parameters: context_length=899, question_length=58, template_length=357, context_preview='So Alice began telling them her adventures from the time when she first saw the White Rabbit. She wa...', question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: format_prompt completed successfully in 0.001 seconds. Output length: 1295 characters\n",
      "INFO: Generating answer with LLM for metadata query\n",
      "INFO: generate_answer called with parameters: prompt_length=1295, prompt_preview='\n",
      "[INST] <<SYS>>\n",
      "You are a clear and concise assistant specialized in summarization and explanation.\n",
      "Use only the provided context to answer the question.\n",
      "If the context does not provide enough informa...'\n",
      "INFO: Invoking LLM for answer generation\n",
      "INFO: generate_answer completed successfully in 30.127 seconds. Answer length: 259, answer_preview='Based on the provided context, the White Rabbit is a character who appears to be a witness in a tria...'\n",
      "INFO: Extracting sources from document metadata\n",
      "INFO: Extracted 3 sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n",
      "INFO: query_with_metadata completed successfully in 30.219 seconds. Answer length: 259, documents_used: 3, sources: 3, similarity_scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401], question: Who is the White Rabbit and how does Alice first m...\n",
      "Similarity threshold: 0.3\n",
      "Documents found: 3\n",
      "Scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401]\n",
      "\n",
      "INFO: query_with_metadata called with parameters: question_length=58, documents_retrieve=3, max_length=500, min_similarity_score=0.500, return_sources=True, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Validating query inputs for metadata query\n",
      "INFO: validate_query_inputs called with parameters: question_length=58, max_length=500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: validate_query_inputs completed successfully in 0.001 seconds. Question validated: length=58\n",
      "INFO: Processing query with metadata: Who is the White Rabbit and how does Alice first m...\n",
      "INFO: Retrieving documents with scores\n",
      "INFO: retrieve_documents_with_scores called with parameters: question_length=58, documents_retrieve=3, min_similarity_score=0.500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Performing similarity search with Chroma DB (with scores)\n",
      "INFO: Filtering documents and scores by similarity threshold: 0.500\n",
      "INFO: retrieve_documents_with_scores completed successfully in 0.030 seconds. Retrieved 3 documents (filtered from 3 total) with scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401]\n",
      "INFO: Building context from 3 retrieved documents\n",
      "INFO: Formatting prompt with template for metadata query\n",
      "INFO: format_prompt called with parameters: context_length=899, question_length=58, template_length=357, context_preview='So Alice began telling them her adventures from the time when she first saw the White Rabbit. She wa...', question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: format_prompt completed successfully in 0.001 seconds. Output length: 1295 characters\n",
      "INFO: Generating answer with LLM for metadata query\n",
      "INFO: generate_answer called with parameters: prompt_length=1295, prompt_preview='\n",
      "[INST] <<SYS>>\n",
      "You are a clear and concise assistant specialized in summarization and explanation.\n",
      "Use only the provided context to answer the question.\n",
      "If the context does not provide enough informa...'\n",
      "INFO: Invoking LLM for answer generation\n",
      "INFO: generate_answer completed successfully in 20.932 seconds. Answer length: 259, answer_preview='Based on the provided context, the White Rabbit is a character who appears to be a witness in a tria...'\n",
      "INFO: Extracting sources from document metadata\n",
      "INFO: Extracted 3 sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n",
      "INFO: query_with_metadata completed successfully in 20.983 seconds. Answer length: 259, documents_used: 3, sources: 3, similarity_scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401], question: Who is the White Rabbit and how does Alice first m...\n",
      "Similarity threshold: 0.5\n",
      "Documents found: 3\n",
      "Scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401]\n",
      "\n",
      "INFO: query_with_metadata called with parameters: question_length=58, documents_retrieve=3, max_length=500, min_similarity_score=0.700, return_sources=True, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Validating query inputs for metadata query\n",
      "INFO: validate_query_inputs called with parameters: question_length=58, max_length=500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: validate_query_inputs completed successfully in 0.001 seconds. Question validated: length=58\n",
      "INFO: Processing query with metadata: Who is the White Rabbit and how does Alice first m...\n",
      "INFO: Retrieving documents with scores\n",
      "INFO: retrieve_documents_with_scores called with parameters: question_length=58, documents_retrieve=3, min_similarity_score=0.700, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Performing similarity search with Chroma DB (with scores)\n",
      "INFO: Filtering documents and scores by similarity threshold: 0.700\n",
      "INFO: retrieve_documents_with_scores completed successfully in 0.031 seconds. Retrieved 1 documents (filtered from 3 total) with scores: [0.7235418775839446]\n",
      "INFO: Building context from 1 retrieved documents\n",
      "INFO: Formatting prompt with template for metadata query\n",
      "INFO: format_prompt called with parameters: context_length=299, question_length=58, template_length=357, context_preview='So Alice began telling them her adventures from the time when she first saw the White Rabbit. She wa...', question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: format_prompt completed successfully in 0.002 seconds. Output length: 695 characters\n",
      "INFO: Generating answer with LLM for metadata query\n",
      "INFO: generate_answer called with parameters: prompt_length=695, prompt_preview='\n",
      "[INST] <<SYS>>\n",
      "You are a clear and concise assistant specialized in summarization and explanation.\n",
      "Use only the provided context to answer the question.\n",
      "If the context does not provide enough informa...'\n",
      "INFO: Invoking LLM for answer generation\n",
      "INFO: generate_answer completed successfully in 27.748 seconds. Answer length: 332, answer_preview='The context provides enough information to answer your question. According to the passage, the White...'\n",
      "INFO: Extracting sources from document metadata\n",
      "INFO: Extracted 1 sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n",
      "INFO: query_with_metadata completed successfully in 27.798 seconds. Answer length: 332, documents_used: 1, sources: 1, similarity_scores: [0.7235418775839446], question: Who is the White Rabbit and how does Alice first m...\n",
      "Similarity threshold: 0.7\n",
      "Documents found: 1\n",
      "Scores: [0.7235418775839446]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test different configuration parameters\n",
    "import time\n",
    "\n",
    "print(\"=== Configuration and Performance Test ===\")\n",
    "\n",
    "# Test with different number of documents retrieved\n",
    "print(\"\\n--- Testing different document retrieval counts ---\")\n",
    "for doc_count in [1, 3, 5]:\n",
    "    start_time = time.time()\n",
    "    response = engine.query_with_metadata(\n",
    "        query_text, \n",
    "        documents_retrieve=doc_count\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Documents retrieved: {doc_count}\")\n",
    "    print(f\"Response time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Answer length: {len(response.answer)} characters\")\n",
    "    print(f\"Actual docs found: {response.retrieved_docs}\")\n",
    "    print()\n",
    "\n",
    "# Test with different similarity thresholds\n",
    "print(\"--- Testing different similarity thresholds ---\")\n",
    "for threshold in [0.3, 0.5, 0.7]:\n",
    "    response = engine.query_with_metadata(\n",
    "        query_text, \n",
    "        min_similarity_score=threshold\n",
    "    )\n",
    "    print(f\"Similarity threshold: {threshold}\")\n",
    "    print(f\"Documents found: {response.retrieved_docs}\")\n",
    "    print(f\"Scores: {response.similarity_scores}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b12332",
   "metadata": {},
   "source": [
    "# Test 6: Error Handling and Edge Cases\n",
    "\n",
    "Test the engine's robustness with edge cases and error conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9288e49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Error Handling and Edge Cases Test ===\n",
      "--- Test 1: Empty query ---\n",
      "INFO: query called with parameters: question_length=0, documents_retrieve=3, max_length=500, min_similarity_score=0.500, question_preview=''\n",
      "INFO: Validating query inputs\n",
      "INFO: validate_query_inputs called with parameters: question_length=0, max_length=500, question_preview=''\n",
      "ERROR: validate_query_inputs failed after 0.001 seconds: Question cannot be empty\n",
      "ERROR: validate_query_inputs encountered unexpected error after 0.003 seconds: Question cannot be empty. Parameters: question_length=0, max_length=500\n",
      "ERROR: query validation error after 0.009 seconds: Question cannot be empty. Question: ...\n",
      "Expected error caught: Question cannot be empty\n",
      "\n",
      "--- Test 2: Very long query ---\n",
      "INFO: query called with parameters: question_length=3500, documents_retrieve=3, max_length=500, min_similarity_score=0.500, question_preview='What is Alice in Wonderland about? What is Alice in Wonderland about? What is Alice in Wonderland ab...'\n",
      "INFO: Validating query inputs\n",
      "INFO: validate_query_inputs called with parameters: question_length=3500, max_length=500, question_preview='What is Alice in Wonderland about? What is Alice in Wonderland about? What is Alice in Wonderland ab...'\n",
      "ERROR: validate_query_inputs failed after 0.002 seconds: Question too long. Maximum 500 characters allowed. Question length: 3500\n",
      "ERROR: validate_query_inputs encountered unexpected error after 0.005 seconds: Question too long. Maximum 500 characters allowed. Parameters: question_length=3500, max_length=500\n",
      "ERROR: query validation error after 0.011 seconds: Question too long. Maximum 500 characters allowed. Question: What is Alice in Wonderland about? What is Alice i...\n",
      "Expected error caught: Question too long. Maximum 500 characters allowed\n",
      "\n",
      "--- Test 3: Query with high similarity threshold ---\n",
      "INFO: query_with_metadata called with parameters: question_length=67, documents_retrieve=3, max_length=500, min_similarity_score=0.900, return_sources=True, question_preview='Completely unrelated query about quantum physics and rocket science'\n",
      "INFO: Validating query inputs for metadata query\n",
      "INFO: validate_query_inputs called with parameters: question_length=67, max_length=500, question_preview='Completely unrelated query about quantum physics and rocket science'\n",
      "INFO: validate_query_inputs completed successfully in 0.002 seconds. Question validated: length=67\n",
      "INFO: Processing query with metadata: Completely unrelated query about quantum physics a...\n",
      "INFO: Retrieving documents with scores\n",
      "INFO: retrieve_documents_with_scores called with parameters: question_length=67, documents_retrieve=3, min_similarity_score=0.900, question_preview='Completely unrelated query about quantum physics and rocket science'\n",
      "INFO: Performing similarity search with Chroma DB (with scores)\n",
      "INFO: Filtering documents and scores by similarity threshold: 0.900\n",
      "INFO: retrieve_documents_with_scores completed successfully in 0.040 seconds. Retrieved 0 documents (filtered from 3 total) with scores: []\n",
      "WARNING: No relevant documents retrieved for metadata query after 0.051 seconds. Returning fallback response\n",
      "Response: I couldn't find any relevant information to answer your question.\n",
      "Documents found: 0\n",
      "\n",
      "--- Test 4: Zero documents requested ---\n",
      "INFO: query_with_metadata called with parameters: question_length=58, documents_retrieve=0, max_length=500, min_similarity_score=0.500, return_sources=True, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Validating query inputs for metadata query\n",
      "INFO: validate_query_inputs called with parameters: question_length=58, max_length=500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: validate_query_inputs completed successfully in 0.001 seconds. Question validated: length=58\n",
      "ERROR: query_with_metadata validation failed after 0.004 seconds: Number of documents (documents_retrieve) must be positive. documents_retrieve=0\n",
      "ERROR: query_with_metadata validation error after 0.006 seconds: Number of documents (documents_retrieve) must be positive. Question: Who is the White Rabbit and how does Alice first m...\n",
      "Expected error caught: Number of documents (documents_retrieve) must be positive\n",
      "\n",
      "=== Edge cases testing completed ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test edge cases and error handling\n",
    "print(\"=== Error Handling and Edge Cases Test ===\")\n",
    "\n",
    "# Test 1: Empty query\n",
    "print(\"--- Test 1: Empty query ---\")\n",
    "try:\n",
    "    response = engine.query(\"\")\n",
    "    print(f\"Unexpected success: {response}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Expected error caught: {e}\")\n",
    "\n",
    "# Test 2: Very long query\n",
    "print(\"\\n--- Test 2: Very long query ---\")\n",
    "long_query = \"What is Alice in Wonderland about? \" * 100  # Very long query\n",
    "try:\n",
    "    response = engine.query(long_query)\n",
    "    print(f\"Unexpected success with long query\")\n",
    "except ValueError as e:\n",
    "    print(f\"Expected error caught: {e}\")\n",
    "\n",
    "# Test 3: Query with no relevant documents (very high similarity threshold)\n",
    "print(\"\\n--- Test 3: Query with high similarity threshold ---\")\n",
    "response = engine.query_with_metadata(\n",
    "    \"Completely unrelated query about quantum physics and rocket science\",\n",
    "    min_similarity_score=0.9\n",
    ")\n",
    "print(f\"Response: {response.answer}\")\n",
    "print(f\"Documents found: {response.retrieved_docs}\")\n",
    "\n",
    "# Test 4: Query with zero documents requested\n",
    "print(\"\\n--- Test 4: Zero documents requested ---\")\n",
    "try:\n",
    "    response = engine.query_with_metadata(query_text, documents_retrieve=0)\n",
    "    print(f\"Unexpected success: {response}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Expected error caught: {e}\")\n",
    "\n",
    "print(\"\\n=== Edge cases testing completed ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdda6927",
   "metadata": {},
   "source": [
    "# Summary and Comparison\n",
    "\n",
    "Summary of the tests and comparison with notebook 002 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341d1f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL SUMMARY ===\n",
      "RAG Query Engine: RAGQueryEngine(model='TheBloke/Llama-2-7B-Chat-GGUF', collection='books')\n",
      "INFO: get_config_info called\n",
      "INFO: get_config_info completed successfully in 0.017 seconds. Config keys: ['model', 'model_type', 'chroma_path', 'collection_name', 'llm_config']\n",
      "Engine Configuration: {'model': 'TheBloke/Llama-2-7B-Chat-GGUF', 'model_type': 'llama', 'chroma_path': '/home/lealdx/personal/rag_chat_project/data/chroma-db/books', 'collection_name': 'books', 'llm_config': {'max_new_tokens': 256, 'temperature': 0.3, 'top_k': 40, 'top_p': 0.9, 'repetition_penalty': 1.05, 'stop': ['</s>', '[/INST]'], 'context_length': 2048, 'threads': 4, 'seed': 42, 'stream': True}}\n",
      "\n",
      "=== Final Test with Original Query ===\n",
      "Query: Who is the White Rabbit and how does Alice first meet him?\n",
      "INFO: query_with_metadata called with parameters: question_length=58, documents_retrieve=3, max_length=500, min_similarity_score=0.500, return_sources=True, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Validating query inputs for metadata query\n",
      "INFO: validate_query_inputs called with parameters: question_length=58, max_length=500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: validate_query_inputs completed successfully in 0.005 seconds. Question validated: length=58\n",
      "INFO: Processing query with metadata: Who is the White Rabbit and how does Alice first m...\n",
      "INFO: Retrieving documents with scores\n",
      "INFO: retrieve_documents_with_scores called with parameters: question_length=58, documents_retrieve=3, min_similarity_score=0.500, question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: Performing similarity search with Chroma DB (with scores)\n",
      "INFO: Filtering documents and scores by similarity threshold: 0.500\n",
      "INFO: retrieve_documents_with_scores completed successfully in 0.329 seconds. Retrieved 3 documents (filtered from 3 total) with scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401]\n",
      "INFO: Building context from 3 retrieved documents\n",
      "INFO: Formatting prompt with template for metadata query\n",
      "INFO: format_prompt called with parameters: context_length=899, question_length=58, template_length=357, context_preview='So Alice began telling them her adventures from the time when she first saw the White Rabbit. She wa...', question_preview='Who is the White Rabbit and how does Alice first meet him?'\n",
      "INFO: format_prompt completed successfully in 0.001 seconds. Output length: 1295 characters\n",
      "INFO: Generating answer with LLM for metadata query\n",
      "INFO: generate_answer called with parameters: prompt_length=1295, prompt_preview='\n",
      "[INST] <<SYS>>\n",
      "You are a clear and concise assistant specialized in summarization and explanation.\n",
      "Use only the provided context to answer the question.\n",
      "If the context does not provide enough informa...'\n",
      "INFO: Invoking LLM for answer generation\n",
      "INFO: generate_answer completed successfully in 27.749 seconds. Answer length: 263, answer_preview='Based on the provided context, the answer to your question is:\n",
      "The White Rabbit is a character who a...'\n",
      "INFO: Extracting sources from document metadata\n",
      "INFO: Extracted 3 sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n",
      "INFO: query_with_metadata completed successfully in 28.157 seconds. Answer length: 263, documents_used: 3, sources: 3, similarity_scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401], question: Who is the White Rabbit and how does Alice first m...\n",
      "\n",
      "Final Answer: Based on the provided context, the answer to your question is:\n",
      "The White Rabbit is a character who appears in Alice's adventures. Alice first meets him when he is talking to himself in a hurry, smiling at everything that is said, and goes by without noticing her.\n",
      "Sources: ['/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md', '/home/lealdx/personal/rag_chat_project/data/books/alice_in_wonderland.md']\n",
      "Retrieved Documents: 3\n",
      "Similarity Scores: [0.7235418775839446, 0.6555808008565527, 0.6501221306982401]\n",
      "\n",
      "=== Tests Completed Successfully ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final summary and comparison with notebook 002\n",
    "print(\"=== FINAL SUMMARY ===\")\n",
    "print(f\"RAG Query Engine: {engine}\")\n",
    "print(f\"Engine Configuration: {engine.get_config_info()}\")\n",
    "\n",
    "# Run one final test with the same query from notebook 002\n",
    "print(f\"\\n=== Final Test with Original Query ===\")\n",
    "print(f\"Query: {query_text}\")\n",
    "\n",
    "final_response = engine.query_with_metadata(query_text)\n",
    "print(f\"\\nFinal Answer: {final_response.answer}\")\n",
    "print(f\"Sources: {final_response.sources}\")\n",
    "print(f\"Retrieved Documents: {final_response.retrieved_docs}\")\n",
    "print(f\"Similarity Scores: {final_response.similarity_scores}\")\n",
    "\n",
    "print(\"\\n=== Tests Completed Successfully ===\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
